{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heavy-building",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import utils\n",
    "from utils import read_json_data, write_json_data, write_tfrecords\n",
    "from preprocessing import *\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_TRC = '../data/GermanTRC.json'\n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "SEED = 12345\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_TEST_SPLIT = 0.8\n",
    "CHUNK_SIZE = 2000\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-wallace",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_fc = read_json_data(DATA_PATH)\n",
    "data, max_sent_len = format_germanfc(raw_data_fc, spacy_model, max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_trc = read_json_data(DATA_PATH_TRC)\n",
    "trc_data, max_sent_len = format_germantrc(raw_data_trc, spacy_model, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-zimbabwe",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling_stats(raw_data, data):\n",
    "    statement_stats = 0\n",
    "    for a in raw_data:\n",
    "        for number in ['1','2','3']:\n",
    "            if 'False_Statement_' + number in a:\n",
    "                if a['False_Statement_' + number] != '':\n",
    "                    statement_stats += 1\n",
    "            if 'True_Statement_' + number in a:\n",
    "                if a['True_Statement_' + number] != '':\n",
    "                    statement_stats += 1\n",
    "\n",
    "    cf_stats = len(list(filter(lambda d: d['lbl'], data))) \n",
    "    print(\"Number of all sentences {}\".format(len(data)))\n",
    "    print(\"True number of statements {}\".format(statement_stats))\n",
    "    print(\"Classified number of statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / statement_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_stats(raw_data_fc, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda lst: [y for x in lst for y in x]\n",
    "raw_data_trc_f = flatten([raw_data[id_raw][1] for id_raw in raw_data_trc.keys()])\n",
    "labeling_stats(raw_data_trc_f, trc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-juvenile",
   "metadata": {},
   "source": [
    "## Seperating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, DATASET_TEST_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-source",
   "metadata": {},
   "source": [
    "### Serialization of formatted data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_data(train_data, DATA_PATH_FORMATED_TRAIN)\n",
    "write_json_data(test_data, DATA_PATH_FORMATED_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-night",
   "metadata": {},
   "source": [
    "## Processing Data Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_hansen(train_data, max_sent_len, w2v_model, spacy_model)\n",
    "test_data = process_hansen(test_data, max_sent_len, w2v_model, spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[0]['processed'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "trc_data = process_hansen(trc_data, max_sent_len, w2v_model, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-optimum",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-ivory",
   "metadata": {},
   "source": [
    "### Serialization Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_train = ['processed', 'lbl']\n",
    "data_keys_test = ['article_id', 'processed', 'lbl']\n",
    "\n",
    "def feature_func_train(ex):\n",
    "    x, y = ex\n",
    "    return {'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "def feature_func_test(ex):\n",
    "    aid, x, y = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data,CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BASE', data_keys_train, feature_func_train)\n",
    "write_tfrecords(test_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TEST_BASE', data_keys_test, feature_func_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-headline",
   "metadata": {},
   "source": [
    "## Contrastive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, assign_bert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, assign_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-hopkins",
   "metadata": {},
   "source": [
    "## Contrastive Sampling with True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, trc_data, assign_bert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, trc_data, assign_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-democrat",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = ['processed', 'lbl', 'cs']\n",
    "\n",
    "def feature_func(ex):\n",
    "    x, y, cs = ex\n",
    "    return {'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\n",
    "            'cs': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(cs).flatten()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_SAMPLING', data_keys, feature_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_TRUENEWS', data_keys, feature_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-yeast",
   "metadata": {},
   "source": [
    "## Processing data BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_bert(train_data, max_sent_len, bert_tokenizer)\n",
    "test_data = process_bert(test_data, max_sent_len, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "trc_data = process_bert(trc_data, max_sent_len, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_train = ['input_ids', 'token_type_ids', 'attention_mask', 'lbl']\n",
    "data_keys_test = ['article_id', 'input_ids', 'token_type_ids', 'attention_mask', 'lbl']\n",
    "\n",
    "def feature_func_train(ex):\n",
    "    inp_ids, token_ids, att_mask, y = ex\n",
    "    return {'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=inp_ids)),\n",
    "            'token_type_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=token_ids)),\n",
    "            'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=att_mask)),           \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "def feature_func_test(ex):\n",
    "    aid, inp_ids, token_ids, att_mask, y = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=inp_ids)),\n",
    "            'token_type_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=token_ids)),\n",
    "            'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=att_mask)),           \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT_BASE', data_keys_train, feature_func_train)\n",
    "write_tfrecords(test_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TEST_BERT_BASE', data_keys_test, feature_func_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-tyler",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data\n",
    "\n",
    "Data has to undergo contrastive sampling after beeing processed for Hansen et al. implementation.\n",
    "Only then can this step be carried out, because a sentence embedding is used to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_sampling = ['input_ids1', 'token_type_ids1', 'attention_mask1',\n",
    "                      'input_ids2', 'token_type_ids2', 'attention_mask2',\n",
    "                      'lbl']\n",
    "\n",
    "def feature_func_sampling(ex):\n",
    "    inp_ids1, token_ids1, att_mask1, inp_ids2, token_ids2, att_mask2, y = ex\n",
    "    feature_i64 = lambda x: tf.train.Feature(int64_list=tf.train.Int64List(value=x))\n",
    "    return {'input_ids1': feature_i64(inp_ids1),\n",
    "            'token_type_ids1': feature_i64(token_ids1),\n",
    "            'attention_mask1': feature_i64(att_mask1),   \n",
    "            'input_ids2': feature_i64(inp_ids2),\n",
    "            'token_type_ids2': feature_i64(token_ids2),\n",
    "            'attention_mask2': feature_i64(att_mask2), \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT_SAMPLING', data_keys_sampling, feature_func_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT_TRUENEWS', data_keys_sampling, feature_func_sampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
