{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facial-exploration",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "identical-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import utils\n",
    "from utils import read_json_data, write_json_data, write_tfrecords\n",
    "from preprocessing import *\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "SEED = 12345\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_TRAIN_SPLIT = 0.8\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "CHUNK_SIZE = 2000\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-lecture",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inside-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = read_json_data(DATA_PATH)\n",
    "data, max_sent_len = format_germanfc(raw_data, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-consciousness",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "previous-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences 14062\n",
      "True number of false statements 974\n",
      "Classified number of false statements 1030 (105.7%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in raw_data:\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-comparative",
   "metadata": {},
   "source": [
    "## Seperating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assumed-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, DATASET_TRAIN_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-arbitration",
   "metadata": {},
   "source": [
    "### Serialization of formatted data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_data(train_data, DATA_PATH_FORMATED_TRAIN)\n",
    "write_json_data(test_data, DATA_PATH_FORMATED_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-curve",
   "metadata": {},
   "source": [
    "## Processing Data Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smaller-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_hansen(train_data, max_sent_len, w2v_model, spacy_model)\n",
    "test_data = process_hansen(test_data, max_sent_len, w2v_model, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-london",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-monthly",
   "metadata": {},
   "source": [
    "### Serialization Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "future-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_train = ['processed', 'lbl']\n",
    "data_keys_test = ['article_id', 'processed', 'lbl']\n",
    "\n",
    "def feature_func_train(ex):\n",
    "    x, y = ex\n",
    "    return {'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "def feature_func_test(ex):\n",
    "    aid, x, y = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data,CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BASE', data_keys_train, feature_func_train)\n",
    "write_tfrecords(test_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TEST_BASE', data_keys_test, feature_func_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-sauce",
   "metadata": {},
   "source": [
    "## Contrastive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suited-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-feature",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "thousand-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = ['processed', 'lbl', 'cs']\n",
    "\n",
    "def feature_func(ex):\n",
    "    x, y, cs = ex\n",
    "    return {'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\n",
    "            'cs': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(cs).flatten()))}\n",
    "\n",
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_SAMPLING', data_keys, feature_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-adult",
   "metadata": {},
   "source": [
    "## Processing data BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "material-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_bert(train_data, max_sent_len, bert_tokenizer)\n",
    "test_data = process_bert(test_data, max_sent_len, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_train = ['input_ids', 'token_type_ids', 'attention_mask', 'lbl']\n",
    "data_keys_test = ['article_id', 'input_ids', 'token_type_ids', 'attention_mask', 'lbl']\n",
    "\n",
    "def feature_func_train(ex):\n",
    "    inp_ids, token_ids, att_mask, y = ex\n",
    "    return {'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=inp_ids)),\n",
    "            'token_type_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=token_ids)),\n",
    "            'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=att_mask)),           \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "def feature_func_test(ex):\n",
    "    aid, inp_ids, token_ids, att_mask, y = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=inp_ids)),\n",
    "            'token_type_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=token_ids)),\n",
    "            'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=att_mask)),           \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data_bert, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT', data_keys_train, feature_func_train)\n",
    "write_tfrecords(test_data_bert, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TEST_BERT', data_keys_test, feature_func_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-handy",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data\n",
    "\n",
    "Data has to undergo contrastive sampling after beeing processed for Hansen et al. implementation.\n",
    "Only then can this step be carried out, because a sentence embedding is used to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distant-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data_sampling)):\n",
    "    cs_ix  = train_data_sampling[i]['cs_ix']\n",
    "    \n",
    "    # rename field dict fields\n",
    "    train_data_sampling[i]['input_ids1'] = train_data_sampling[i].pop('input_ids')\n",
    "    train_data_sampling[i]['token_type_ids1'] = train_data_sampling[i].pop('token_type_ids')\n",
    "    train_data_sampling[i]['attention_mask1'] = train_data_sampling[i].pop('attention_mask')\n",
    "    \n",
    "    train_data_sampling[i]['input_ids2'] = train_data[cs_ix]['input_ids']\n",
    "    train_data_sampling[i]['token_type_ids2'] = train_data[cs_ix]['token_type_ids']\n",
    "    train_data_sampling[i]['attention_mask2'] = train_data[cs_ix]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "looking-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_sampling = ['input_ids1', 'token_type_ids1', 'attention_mask1',\n",
    "                      'input_ids2', 'token_type_ids2', 'attention_mask2',\n",
    "                      'lbl']\n",
    "\n",
    "def feature_func_sampling(ex):\n",
    "    inp_ids1, token_ids1, att_mask1, inp_ids2, token_ids2, att_mask2, y = ex\n",
    "    feature_i64 = lambda x: tf.train.Feature(int64_list=tf.train.Int64List(value=x))\n",
    "    return {'input_ids1': feature_i64(inp_ids1),\n",
    "            'token_type_ids1': feature_i64(token_ids1),\n",
    "            'attention_mask1': feature_i64(att_mask1),   \n",
    "            'input_ids2': feature_i64(inp_ids2),\n",
    "            'token_type_ids2': feature_i64(token_ids2),\n",
    "            'attention_mask2': feature_i64(att_mask2), \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT_SAMPLING', data_keys_sampling, feature_func_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-champion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
