{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boring-following",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "desirable-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer\n",
    "from utils import chunks\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "NUM_ARTICLES = 489\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "SEED = 12345\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_TRAIN_SPLIT = 0.8\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "CHUNK_SIZE = 2000\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-subscriber",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "latest-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def count_matches(false_statement, sentence):\n",
    "    count = 0\n",
    "    sent_copy = sentence[:]\n",
    "    for w in false_statement:\n",
    "        if w in sent_copy:\n",
    "            count += 1\n",
    "            sent_copy.remove(w)\n",
    "    return count\n",
    "\n",
    "def process_text(sentences, article_id,  max_sent_len):\n",
    "    processed = []\n",
    "    for s in sentences:\n",
    "        # ignore sentences of length 1\n",
    "        if len(s) <= 1:\n",
    "            continue\n",
    "        # ignore sentences consisting exclusively of punctuation\n",
    "        if not any([not t.is_punct for t in s]):\n",
    "            continue\n",
    "        # ignore sentences not containing any letter\n",
    "        if not any([any([c.isalpha() for c in t.text]) for t in s]):\n",
    "            continue\n",
    "        if len(s) > max_sent_len:\n",
    "            max_sent_len = len(s)\n",
    "        processed.append({\n",
    "            'article_id': article_id,\n",
    "            'org': s.text,\n",
    "            'lbl': 0.0,\n",
    "            'tokenized': [t.text for t in s],\n",
    "            'tokenized_lower': [t.text.lower() for t in s]\n",
    "        })\n",
    "    return processed, max_sent_len\n",
    "\n",
    "data = []\n",
    "max_sent_len = 0\n",
    "for article_id, article in enumerate(read_data(DATA_PATH)):\n",
    "    title = spacy_model(article['Title']).sents\n",
    "    teaser = spacy_model(article['Teaser']).sents\n",
    "    text = spacy_model(article['Text']).sents\n",
    "    \n",
    "    p_title, max_sent_len = process_text(title, article_id, max_sent_len)\n",
    "    p_teaser, max_sent_len = process_text(teaser, article_id, max_sent_len)\n",
    "    p_text, max_sent_len = process_text(text, article_id, max_sent_len)\n",
    "       \n",
    "    article_data = p_title + p_teaser + p_text\n",
    "\n",
    "    # Label sentences\n",
    "    false_statements = [article['False_Statement_1'], article['False_Statement_2'], article['False_Statement_3']]     \n",
    "    for fs in false_statements:\n",
    "        if fs != '':\n",
    "            fs_tokens = [t.text.lower() for t in spacy_model(fs)]\n",
    "            matches = [count_matches(fs_tokens, t) for t in [d['tokenized_lower'] for d in article_data]]\n",
    "            m = max(matches)\n",
    "            max_indexes = [i for i, j in enumerate(matches) if j == m]\n",
    "            \n",
    "            # +++++++ DEBUG CODE - START +++++++++ #\n",
    "            #if article_id == 400:\n",
    "            #    print(\"\\n\\nFalse Statement: {} \\n\\n\".format(fs))\n",
    "            #    for mi in max_indexes:\n",
    "            #        print(article_data[mi]['org'])\n",
    "            # +++++++ DEBUG CODE - END   +++++++++ #\n",
    "                \n",
    "            for mi in max_indexes:\n",
    "                article_data[mi]['lbl'] = 1.0\n",
    "            \n",
    "    data = data + article_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-action",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "directed-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences 14062\n",
      "True number of false statements 974\n",
      "Classified number of false statements 1030 (105.7%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in read_data(DATA_PATH):\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-omaha",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "divided-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_deps(doc, max_sent_len):\n",
    "    oh_vectors = []\n",
    "    for token in doc:\n",
    "        vec = np.zeros(max_sent_len)\n",
    "        vec[token.head.i] = 1\n",
    "        oh_vectors.append(vec)\n",
    "        \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(oh_vectors) < max_sent_len:\n",
    "        oh_vectors.append(np.zeros(max_sent_len))\n",
    "    return oh_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "manufactured-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    doc = spacy_model(d['org'])\n",
    "    d['processed'] = to_deps(doc, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-alberta",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "international-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentence, max_sent_len):\n",
    "    vectorized_sentence = []\n",
    "    vector_dim = w2v_model.wv.vector_size\n",
    "    for word in sentence:\n",
    "        if word in w2v_model.wv:\n",
    "            vectorized_sentence.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            vectorized_sentence.append(np.zeros(vector_dim))\n",
    "            \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(vectorized_sentence) < max_sent_len:\n",
    "        vectorized_sentence.append(np.zeros(vector_dim))\n",
    "        \n",
    "    return vectorized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "interior-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    embedded_words = embed(d['tokenized_lower'], max_sent_len)\n",
    "    d['processed'] = np.concatenate((embedded_words, d['processed']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-seattle",
   "metadata": {},
   "source": [
    "### Seperating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "capable-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is seperated by article because of MAP evaluation later\n",
    "num_train_articles = int(DATASET_TRAIN_SPLIT * NUM_ARTICLES)\n",
    "train_data = list(filter(lambda d: d['article_id'] <= num_train_articles, data))\n",
    "test_data = list(filter(lambda d: d['article_id'] > num_train_articles, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-leadership",
   "metadata": {},
   "source": [
    "### Serialization of formatted data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "touched-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH_FORMATED_TRAIN, 'w') as fout:\n",
    "    json.dump(train_data, fout)\n",
    "    \n",
    "with open(DATA_PATH_FORMATED_TEST, 'w') as fout:\n",
    "    json.dump(test_data, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-carolina",
   "metadata": {},
   "source": [
    "### Contrastive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "outstanding-twins",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_embeddings(data):\n",
    "    word_vector_dim = w2v_model.wv.vector_size\n",
    "    for d in data:\n",
    "        word_embeddings = [w[:word_vector_dim] for w in d['processed']]\n",
    "        yield np.mean(word_embeddings, axis=0)\n",
    "        \n",
    "def retrieve_topk_ixs(entry_index, data, k, sims):\n",
    "    topk_stack = [(0,0)]\n",
    "    \n",
    "    for i, sim in enumerate(sims):\n",
    "        is_greater = any([sim > tk_sim for (index, tk_sim) in topk_stack])\n",
    "        negative_label = data[entry_index]['lbl'] != data[i]['lbl']\n",
    "        not_own_sim = entry_index != i\n",
    "        \n",
    "        if is_greater and negative_label and not_own_sim: \n",
    "            if len(topk_stack) >= k:\n",
    "                topk_stack.pop()\n",
    "\n",
    "            topk_stack.append((i, sim))    \n",
    "            topk_stack.sort(reverse=True)\n",
    "    return [index for (index, sim) in topk_stack]\n",
    "\n",
    "# only use train data\n",
    "# no negative sampling for test data neccesary\n",
    "sentence_embeddings = list(compute_sentence_embeddings(train_data))\n",
    "\n",
    "similarities = cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "k = NUM_SAMPLING_CANDIDATES\n",
    "processed_topk_candidates = []\n",
    "for i, row_sims in enumerate(similarities):\n",
    "    top_k_ixs = retrieve_topk_ixs(i, data, k, row_sims)\n",
    "    \n",
    "    top_k_processed = []    \n",
    "    for top_k_ix in top_k_ixs:\n",
    "        top_k_processed.append(train_data[top_k_ix]['processed']) \n",
    "    processed_topk_candidates.append(top_k_processed)\n",
    "    \n",
    "\n",
    "def assign_candidate(d, ptc):\n",
    "    d_copy = dict(d)\n",
    "    d_copy['cs'] = ptc\n",
    "    return d_copy\n",
    "    \n",
    "train_data = [[assign_candidate(d, ptc) for ptc in ptcs] for d, ptcs in zip(train_data, processed_topk_candidates)]\n",
    "\n",
    "flatten = lambda lst: [j for sub in lst for j in sub]\n",
    "train_data = flatten(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-serbia",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-creek",
   "metadata": {},
   "source": [
    "### Serialization Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adverse-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_wsampling(sdata, chunk_size, file_suffix):\n",
    "    aid_chunks = chunks([d['article_id'] for d in sdata], chunk_size)\n",
    "    X_chunks = chunks([d['processed'] for d in sdata], chunk_size)\n",
    "    y_chunks = chunks([d['lbl'] for d in sdata], chunk_size)\n",
    "    cs_chunks = chunks([d['cs'] for d in sdata], chunk_size)\n",
    "\n",
    "    zipped_chunks = zip(aid_chunks, X_chunks, y_chunks, cs_chunks)\n",
    "    for (i, (aid_chunk, X_chunk, y_chunk, cs_chunk)) in enumerate(zipped_chunks):\n",
    "        writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}_{}'.format(file_suffix, i) + '.tfrecords')\n",
    "        for (aidc, xc, yc, csc) in zip(aid_chunk, X_chunk, y_chunk, cs_chunk):\n",
    "            # Convert to TFRecords and save to file\n",
    "            feature = {\n",
    "                'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aidc])),\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc])),\n",
    "                'cs': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(csc).flatten()))\n",
    "            }\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "        writer.close()\n",
    "        \n",
    "def serialize(sdata, chunk_size, file_suffix):\n",
    "    aid_chunks = chunks([d['article_id'] for d in sdata], chunk_size)\n",
    "    X_chunks = chunks([d['processed'] for d in sdata], chunk_size)\n",
    "    y_chunks = chunks([d['lbl'] for d in sdata], chunk_size)\n",
    "    \n",
    "    zipped_chunks = zip(aid_chunks, X_chunks, y_chunks)\n",
    "    for (i, (aid_chunk, X_chunk, y_chunk)) in enumerate(zipped_chunks):\n",
    "        writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}_{}'.format(file_suffix, i) + '.tfrecords')\n",
    "        for (aidc, xc, yc) in zip(aid_chunk, X_chunk, y_chunk):\n",
    "            # Convert to TFRecords and save to file\n",
    "            feature = {\n",
    "                'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aidc])),\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc]))\n",
    "            }\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-temperature",
   "metadata": {},
   "source": [
    "#### Serialize Base Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "enabling-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_wsampling(train_data, chunk_size, 'TRAIN_SAMPLING')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-helicopter",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "banned-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(train_data, chunk_size, 'TRAIN')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
