{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ignored-cameroon",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ongoing-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import utils\n",
    "from utils import read_json_data, write_json_data, write_tfrecords\n",
    "from preprocessing import *\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_TRC = '../data/GermanTRC.json'\n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "SEED = 12345\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_TRAIN_SPLIT = 0.8\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "CHUNK_SIZE = 2000\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-passport",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "necessary-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dramatic-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = read_json_data(DATA_PATH)\n",
    "data, max_sent_len = format_germanfc(raw_data, spacy_model, max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informational-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = read_json_data(DATA_PATH_TRC)\n",
    "trc_data, max_sent_len = format_germantrc(raw_data, spacy_model, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-angle",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comic-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences 14062\n",
      "True number of false statements 974\n",
      "Classified number of false statements 1030 (105.7%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in raw_data:\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-algebra",
   "metadata": {},
   "source": [
    "## Seperating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "separate-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, DATASET_TRAIN_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-technique",
   "metadata": {},
   "source": [
    "### Serialization of formatted data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_data(train_data, DATA_PATH_FORMATED_TRAIN)\n",
    "write_json_data(test_data, DATA_PATH_FORMATED_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-ribbon",
   "metadata": {},
   "source": [
    "## Processing Data Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "polar-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_hansen(train_data, max_sent_len, w2v_model, spacy_model)\n",
    "test_data = process_hansen(test_data, max_sent_len, w2v_model, spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "trc_data = process_hansen(trc_data, max_sent_len, w2v_model, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-friendly",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-midwest",
   "metadata": {},
   "source": [
    "### Serialization Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "square-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_train = ['processed', 'lbl']\n",
    "data_keys_test = ['article_id', 'processed', 'lbl']\n",
    "\n",
    "def feature_func_train(ex):\n",
    "    x, y = ex\n",
    "    return {'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "def feature_func_test(ex):\n",
    "    aid, x, y = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data,CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BASE', data_keys_train, feature_func_train)\n",
    "write_tfrecords(test_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TEST_BASE', data_keys_test, feature_func_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-venezuela",
   "metadata": {},
   "source": [
    "## Contrastive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, assign_bert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "governing-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, assign_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-england",
   "metadata": {},
   "source": [
    "## Contrastive Sampling with True News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, trc_data, assign_bert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unexpected-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sampling = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES, trc_data, assign_bert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-example",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "artificial-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = ['processed', 'lbl', 'cs']\n",
    "\n",
    "def feature_func(ex):\n",
    "    x, y, cs = ex\n",
    "    return {'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\n",
    "            'cs': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(cs).flatten()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_SAMPLING', data_keys, feature_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "primary-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_TRUENEWS', data_keys, feature_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-vinyl",
   "metadata": {},
   "source": [
    "## Processing data BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "divine-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_bert(train_data, max_sent_len, bert_tokenizer)\n",
    "test_data = process_bert(test_data, max_sent_len, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "promotional-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "trc_data = process_bert(trc_data, max_sent_len, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "textile-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_train = ['input_ids', 'token_type_ids', 'attention_mask', 'lbl']\n",
    "data_keys_test = ['article_id', 'input_ids', 'token_type_ids', 'attention_mask', 'lbl']\n",
    "\n",
    "def feature_func_train(ex):\n",
    "    inp_ids, token_ids, att_mask, y = ex\n",
    "    return {'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=inp_ids)),\n",
    "            'token_type_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=token_ids)),\n",
    "            'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=att_mask)),           \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "def feature_func_test(ex):\n",
    "    aid, inp_ids, token_ids, att_mask, y = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'input_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=inp_ids)),\n",
    "            'token_type_ids': tf.train.Feature(int64_list=tf.train.Int64List(value=token_ids)),\n",
    "            'attention_mask': tf.train.Feature(int64_list=tf.train.Int64List(value=att_mask)),           \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT_BASE', data_keys_train, feature_func_train)\n",
    "write_tfrecords(test_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TEST_BERT_BASE', data_keys_test, feature_func_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-holiday",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data\n",
    "\n",
    "Data has to undergo contrastive sampling after beeing processed for Hansen et al. implementation.\n",
    "Only then can this step be carried out, because a sentence embedding is used to measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "iraqi-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys_sampling = ['input_ids1', 'token_type_ids1', 'attention_mask1',\n",
    "                      'input_ids2', 'token_type_ids2', 'attention_mask2',\n",
    "                      'lbl']\n",
    "\n",
    "def feature_func_sampling(ex):\n",
    "    inp_ids1, token_ids1, att_mask1, inp_ids2, token_ids2, att_mask2, y = ex\n",
    "    feature_i64 = lambda x: tf.train.Feature(int64_list=tf.train.Int64List(value=x))\n",
    "    return {'input_ids1': feature_i64(inp_ids1),\n",
    "            'token_type_ids1': feature_i64(token_ids1),\n",
    "            'attention_mask1': feature_i64(att_mask1),   \n",
    "            'input_ids2': feature_i64(inp_ids2),\n",
    "            'token_type_ids2': feature_i64(token_ids2),\n",
    "            'attention_mask2': feature_i64(att_mask2), \n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT_SAMPLING', data_keys_sampling, feature_func_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bigger-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tfrecords(train_data_sampling, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_BERT_TRUENEWS', data_keys_sampling, feature_func_sampling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
