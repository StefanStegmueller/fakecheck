{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southwest-neutral",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aerial-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from utils import read_json_data, write_json_data, write_tfrecords\n",
    "from preprocessing import *\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "SEED = 12345\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_TRAIN_SPLIT = 0.8\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "CHUNK_SIZE = 2000\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-rebel",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adolescent-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = read_json_data(DATA_PATH)\n",
    "data, max_sent_len = format_germanfc(raw_data, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-offering",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "approved-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences 14062\n",
      "True number of false statements 974\n",
      "Classified number of false statements 1030 (105.7%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in raw_data:\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-messaging",
   "metadata": {},
   "source": [
    "## Seperating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thorough-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(data, DATASET_TRAIN_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-wright",
   "metadata": {},
   "source": [
    "### Serialization of formatted data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-latino",
   "metadata": {},
   "source": [
    "## Processing Data Hansen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "humanitarian-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_hansen(train_data, max_sent_len, w2v_model, spacy_model)\n",
    "test_data = process_hansen(test_data, max_sent_len, w2v_model, spacy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-deposit",
   "metadata": {},
   "source": [
    "## Contrastive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collected-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = contrastive_sampling(train_data, w2v_model, NUM_SAMPLING_CANDIDATES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-there",
   "metadata": {},
   "source": [
    "## Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-moral",
   "metadata": {},
   "source": [
    "### Serialization Hansen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-contributor",
   "metadata": {},
   "source": [
    "#### Serialize Base Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = ['article_id', 'processed', 'lbl']\n",
    "\n",
    "def feature_func(ex):\n",
    "    aid, x, y = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y]))}\n",
    "\n",
    "write_tfrecords(train_data,CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN', data_keys, feature_func)\n",
    "write_tfrecords(test_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TEST', data_keys, feature_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-affiliation",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_keys = ['article_id', 'processed', 'lbl', 'cs']\n",
    "\n",
    "def feature_func(ex):\n",
    "    aid, x, y, cs = ex\n",
    "    return {'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aid])),\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(x).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\n",
    "            'cs': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(cs).flatten()))}\n",
    "\n",
    "write_tfrecords(train_data, CHUNK_SIZE, DATA_PATH_PROCESSED, 'TRAIN_SAMPLING_tt', data_keys, feature_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-stewart",
   "metadata": {},
   "source": [
    "### Serialization BERT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
