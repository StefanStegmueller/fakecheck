{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer, Dropout, LSTM, Dense, InputLayer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "NUM_ARTICLES = 489\n",
    "MODEL_NAME = \"CLEF_2019_HANSEN\"\n",
    "MODEL_PATH_MAIN = '../models/' + MODEL_NAME\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "SEED = 12345\n",
    "LSTM_HIDDEN_UNITS = 100\n",
    "EPOCHS = 10\n",
    "CROSS_VALIDATION_K_FOLDS = 19\n",
    "DATASET_SIZE = 14765\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(MODEL_PATH_SPACY, disable=[\"vocab\"])\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "False Statement: Katalanischer Regierungschef unterzeichnet Unabhängigkeit: \"Keine rechtliche Bedeutung\"\n",
      " \n",
      "\n",
      "\n",
      "Katalanischer Regierungschef unterzeichnet Unabhängigkeit:\n",
      "\"Keine rechtliche Bedeutung\"\n",
      "\n",
      "\n",
      "False Statement: Die Unterzeichnung hat stattgefunden, obwohl Carles Puigdemont der spanischen Presse zufolge dem Regionalparlament vorgeschlagen hatte, die Unabhängigkeitserklärung zu verschieben, um einem Dialog mit Madrid einzuleiten.\n",
      " \n",
      "\n",
      "\n",
      "Die Unterzeichnung hat stattgefunden, obwohl Carles Puigdemont der spanischen Presse zufolge dem Regionalparlament vorgeschlagen hatte, die Unabhängigkeitserklärung zu verschieben, um einem Dialog mit Madrid einzuleiten.  \n"
     ]
    }
   ],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def count_matches(false_statement, sentence):\n",
    "    count = 0\n",
    "    sent_copy = sentence[:]\n",
    "    for w in false_statement:\n",
    "        if w in sent_copy:\n",
    "            count += 1\n",
    "            sent_copy.remove(w)\n",
    "    return count\n",
    "\n",
    "\n",
    "data = []\n",
    "max_sent_len = 0\n",
    "for article_id, article in enumerate(read_data(DATA_PATH)):\n",
    "    # Concatenate article text\n",
    "    text = article['Title'] + article['Teaser'] + article['Text']\n",
    "        \n",
    "    sentences = spacy_model(text).sents\n",
    "    article_data = []\n",
    "    for s in sentences:\n",
    "        if len(s) > max_sent_len:\n",
    "            max_sent_len = len(s)\n",
    "        article_data.append({\n",
    "            'article_id': article_id,\n",
    "            'org': s.text,\n",
    "            'lbl': True,\n",
    "            'tokenized': [t.text for t in s],\n",
    "            'tokenized_lower': [t.text.lower() for t in s]\n",
    "        })\n",
    " \n",
    "    # Label sentences\n",
    "    # The sentences matching the most tokens with a false statement will be labeled as False\n",
    "    false_statements = [article['False_Statement_1'], article['False_Statement_2'], article['False_Statement_3']]     \n",
    "    for fs in false_statements:\n",
    "        if fs != '':\n",
    "            fs_tokens = [t.text.lower() for t in spacy_model(fs)]\n",
    "            matches = [count_matches(fs_tokens, t) for t in [d['tokenized_lower'] for d in article_data]]\n",
    "            m = max(matches)\n",
    "            max_indexes = [i for i, j in enumerate(matches) if j == m]\n",
    "            \n",
    "            # +++++++ DEBUG CODE - START +++++++++ #\n",
    "            if article_id == 400:\n",
    "                print(\"\\n\\nFalse Statement: {} \\n\\n\".format(fs))\n",
    "                for mi in max_indexes:\n",
    "                    print(article_data[mi]['org'])\n",
    "            # +++++++ DEBUG CODE - END   +++++++++ #\n",
    "                \n",
    "            for mi in max_indexes:\n",
    "                article_data[mi]['lbl'] = False\n",
    "            \n",
    "    data = data + article_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences 14765\n",
      "True number of false statements 974\n",
      "Classified number of false statements 1022 (104.9%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in read_data(DATA_PATH):\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: not d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_deps(doc, max_sent_len):\n",
    "    oh_vectors = []\n",
    "    for token in doc:\n",
    "        vec = np.zeros(max_sent_len)\n",
    "        vec[token.head.i] = 1\n",
    "        oh_vectors.append(vec)\n",
    "        \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(oh_vectors) < max_sent_len:\n",
    "        oh_vectors.append(np.zeros(max_sent_len))\n",
    "    return oh_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    doc = spacy_model(d['org'])\n",
    "    d['processed'] = to_deps(doc, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentence, max_sent_len):\n",
    "    vectorized_sentence = []\n",
    "    vector_dim = w2v_model.wv.vector_size\n",
    "    for word in sentence:\n",
    "        if word in w2v_model.wv:\n",
    "            vectorized_sentence.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            vectorized_sentence.append(np.zeros(vector_dim))\n",
    "            \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(vectorized_sentence) < max_sent_len:\n",
    "        vectorized_sentence.append(np.zeros(vector_dim))\n",
    "        \n",
    "    return vectorized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    d['processed'] = np.concatenate((embed(d['tokenized_lower'], max_sent_len), d['processed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are 0 for true and 1 for false statements\n",
    "y = [0.0 if d['lbl'] == True else 1.0 for d in data]\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "chunk_size = 2000\n",
    "id_chunks = chunks([d['article_id'] for d in data], chunk_size)\n",
    "X_chunks = chunks([d['processed'] for d in data], chunk_size)\n",
    "y_chunks = chunks(y, chunk_size)\n",
    "\n",
    "for (i, (id_chunk, X_chunk, y_chunk)) in enumerate(zip(id_chunks, X_chunks, y_chunks)):\n",
    "    writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}'.format(i) + '.tfrecords')\n",
    "    for (idc, xc, yc) in zip(id_chunk, X_chunk, y_chunk):\n",
    "        # Convert to TFRecords and save to file\n",
    "        feature = {\n",
    "            'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[idc])),\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_parser at 0x7f9fbc7187a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function input_parser at 0x7f9fbc7187a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f9fce3415f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f9fce3415f0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f9fce341f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f9fce341f80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f9fbbeed200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f9fbbeed200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n"
     ]
    }
   ],
   "source": [
    "def input_parser(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64), \n",
    "                           'x': tf.io.FixedLenFeature([135, 285], dtype=tf.float32),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed_example = tf.io.parse_single_example(example, feature_description)\n",
    "    return (parsed_example['article_id'], parsed_example['x'], parsed_example['y'])\n",
    "\n",
    "data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_*.tfrecords')\n",
    "raw_dataset = tf.data.TFRecordDataset(data_files)\n",
    "dataset = raw_dataset.map(input_parser)\n",
    "\n",
    "num_train_articles = int(0.8 * NUM_ARTICLES)\n",
    "num_train_articles = tf.constant(num_train_articles, dtype=tf.int64)\n",
    "\n",
    "\n",
    "rem_article_id = lambda ida, x, y: (x, y)\n",
    "is_train_example = lambda ida, x, y: tf.squeeze(tf.math.less_equal(ida, num_train_articles))\n",
    "is_no_train_example = lambda ida, x, y: tf.squeeze(tf.math.greater(ida, num_train_articles))\n",
    "\n",
    "# model examples do not contain article_id\n",
    "train_dataset = dataset.filter(is_train_example)\n",
    "train_dataset = train_dataset.map(rem_article_id).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = dataset.filter(is_no_train_example)\n",
    "test_dataset = test_dataset.map(rem_article_id).batch(BATCH_SIZE)\n",
    "\n",
    "# the eval example do contain article_id to determine MAP\n",
    "test_dataset_eval = dataset.filter(is_no_train_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without ranking loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOURCE: https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'return_attention': self.return_attention,\n",
    "            'init': self.init,\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inp_shape = (135, 285)\n",
    "    model = Sequential(name='simple')\n",
    "    model.add(LSTM(LSTM_HIDDEN_UNITS, input_shape=inp_shape, return_sequences = True, name='lstm'))\n",
    "    model.add(Attention(name='attention'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid', name='dense'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for (fold, (train_index, val_index)) in enumerate(KFold(CROSS_VALIDATION_K_FOLDS).split(train_dataset)):\n",
    "    X_train_fold, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "    \n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH_MAIN + \"_FOLD_{}\".format(fold), \n",
    "                                                             monitor='val_accuracy', verbose=1, \n",
    "                                                             save_best_only=True, mode='max')\n",
    "    \n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(x=X_train_fold, y=y_train_fold,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "                validation_data=(X_val, y_val))\n",
    "    \n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f9fca1433d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f9fca1433d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "Model: \"simple\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 135, 100)          154400    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 100)               235       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 154,736\n",
      "Trainable params: 154,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "     96/Unknown - 33s 343ms/step - loss: 0.3052 - accuracy: 0.9241 - precision: 0.0769 - recall: 0.0061\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.93974, saving model to ../models/CLEF_2019_HANSEN\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9fc5cfa710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f9fc5cfa710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f9fca1433d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f9fca1433d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /nix/store/5585rs1zvh0jlxn2jlknsawqcd7pnkhh-python3-3.7.7-env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN/assets\n",
      "96/96 [==============================] - 51s 532ms/step - loss: 0.3052 - accuracy: 0.9241 - precision: 0.0769 - recall: 0.0061 - val_loss: 0.2255 - val_accuracy: 0.9397 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.9286 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Epoch 00002: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 37s 390ms/step - loss: 0.2483 - accuracy: 0.9287 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.2160 - val_accuracy: 0.9382 - val_precision: 0.1429 - val_recall: 0.0051\n",
      "Epoch 3/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9282 - precision: 0.3529 - recall: 0.0074\n",
      "Epoch 00003: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 36s 376ms/step - loss: 0.2376 - accuracy: 0.9285 - precision: 0.3529 - recall: 0.0074 - val_loss: 0.2204 - val_accuracy: 0.9397 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.2262 - accuracy: 0.9291 - precision: 0.5882 - recall: 0.0123\n",
      "Epoch 00004: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 37s 383ms/step - loss: 0.2260 - accuracy: 0.9292 - precision: 0.5882 - recall: 0.0123 - val_loss: 0.2077 - val_accuracy: 0.9391 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.2168 - accuracy: 0.9296 - precision: 0.5909 - recall: 0.0321\n",
      "Epoch 00005: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 37s 385ms/step - loss: 0.2167 - accuracy: 0.9294 - precision: 0.5652 - recall: 0.0319 - val_loss: 0.2078 - val_accuracy: 0.9361 - val_precision: 0.3696 - val_recall: 0.0859\n",
      "Epoch 6/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.2065 - accuracy: 0.9300 - precision: 0.5490 - recall: 0.0693\n",
      "Epoch 00006: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 38s 397ms/step - loss: 0.2072 - accuracy: 0.9298 - precision: 0.5490 - recall: 0.0686 - val_loss: 0.2101 - val_accuracy: 0.9388 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.1940 - accuracy: 0.9328 - precision: 0.6691 - recall: 0.1121\n",
      "Epoch 00007: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 35s 369ms/step - loss: 0.1933 - accuracy: 0.9328 - precision: 0.6642 - recall: 0.1115 - val_loss: 0.2062 - val_accuracy: 0.9388 - val_precision: 0.4118 - val_recall: 0.0354\n",
      "Epoch 8/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9380 - precision: 0.7066 - recall: 0.2119\n",
      "Epoch 00008: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 40s 414ms/step - loss: 0.1739 - accuracy: 0.9376 - precision: 0.7066 - recall: 0.2096 - val_loss: 0.2163 - val_accuracy: 0.9382 - val_precision: 0.3333 - val_recall: 0.0253\n",
      "Epoch 9/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9424 - precision: 0.6967 - recall: 0.3346\n",
      "Epoch 00009: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 38s 399ms/step - loss: 0.1521 - accuracy: 0.9422 - precision: 0.6967 - recall: 0.3321 - val_loss: 0.2424 - val_accuracy: 0.9267 - val_precision: 0.2529 - val_recall: 0.1111\n",
      "Epoch 10/10\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9528 - precision: 0.7425 - recall: 0.5154\n",
      "Epoch 00010: val_accuracy did not improve from 0.93974\n",
      "96/96 [==============================] - 44s 455ms/step - loss: 0.1279 - accuracy: 0.9529 - precision: 0.7434 - recall: 0.5147 - val_loss: 0.2269 - val_accuracy: 0.9367 - val_precision: 0.3438 - val_recall: 0.0556\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH_MAIN, \n",
    "                                                         monitor='val_accuracy', verbose=1, \n",
    "                                                         save_best_only=True, mode='max')\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "            validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "test_model = tf.keras.models.load_model(MODEL_PATH_MAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "false_statement = \"Um die Ermordung unschuldiger Zivilisten in Russland zu üben, sucht die NATO für ihre Manöver russischsprachige Menschen.\"\n",
    "tokens = spacy_model(false_statement)\n",
    "deps = to_deps(tokens, 135)\n",
    "word_vecs = embed([t.text.lower() for t in tokens], 135)\n",
    "inp = np.concatenate((word_vecs, deps), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07572758]]\n"
     ]
    }
   ],
   "source": [
    "prediction = test_model.predict(np.array( [inp,] ))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = list(test_dataset_eval.as_numpy_iterator())\n",
    "eval_data = [(ida[0], x, y[0]) for ida, x, y in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.36947520000306827\n"
     ]
    }
   ],
   "source": [
    "# MAP metric is based on the official CLEF2019 implementation: \n",
    "# https://github.com/apepa/clef2019-factchecking-task1/blob/7d463336897ad1f870cb6a481953b94550c788a7/scorer/main.py#L52\n",
    "\n",
    "def mean_average_precision(data):\n",
    "    avg_precisions = []\n",
    "    article_ids = set([ida for ida, _, _ in data])\n",
    "    num_articles = len(article_ids)\n",
    "    \n",
    "    for id_article in article_ids:\n",
    "        article_examples = [(x,y) for ida, x, y in data if ida == id_article]\n",
    "        xs = [x for x,y in article_examples]\n",
    "        ys = [y for x,y in article_examples]\n",
    "        \n",
    "        num_positive = sum(ys)\n",
    "\n",
    "        predictions = [p[0] for p in test_model.predict(np.array(xs))]\n",
    "        ranked_indices = [i for i, v in sorted(enumerate(predictions), key=lambda tup: tup[1], reverse=True)]\n",
    "        \n",
    "        # ++++ DEBUG CODE - START +++ #\n",
    "        #hits = []\n",
    "        #for i in range(len(ranked_indices)):\n",
    "        #   if ys[ranked_indices[i]] == 1:\n",
    "        #        hits.append(1)\n",
    "        #    else:\n",
    "        #        hits.append(0)\n",
    "        #print(hits)\n",
    "        # ++++ DEBUG CODE - END   +++ #\n",
    "        \n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        for i in range(len(ranked_indices)):\n",
    "            if ys[ranked_indices[i]] == 1:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "            \n",
    "        if precisions:\n",
    "            avg_precisions.append(sum(precisions) / num_positive)\n",
    "        else:\n",
    "            avg_precisions.append(0)\n",
    "        \n",
    "    return sum(avg_precisions) / num_articles\n",
    "    \n",
    "print('MAP: {}'.format(mean_average_precision(eval_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 - python",
   "language": "python",
   "name": "ipython_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
