{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Layer, Dropout, LSTM, Dense, InputLayer\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "NUM_ARTICLES = 489\n",
    "MODEL_NAME = \"CLEF_2019_HANSEN\"\n",
    "MODEL_PATH_MAIN = '../models/' + MODEL_NAME\n",
    "MODEL_PATH_RANKING = '../models/' + MODEL_NAME + '_RANKING'\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "SEED = 12345\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "LSTM_HIDDEN_UNITS = 100\n",
    "EPOCHS = 10\n",
    "CROSS_VALIDATION_K_FOLDS = 19\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_TRAIN_SPLIT = 0.8\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "BATCH_SIZE = 120\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(MODEL_PATH_SPACY, disable=[\"vocab\"])\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def count_matches(false_statement, sentence):\n",
    "    count = 0\n",
    "    sent_copy = sentence[:]\n",
    "    for w in false_statement:\n",
    "        if w in sent_copy:\n",
    "            count += 1\n",
    "            sent_copy.remove(w)\n",
    "    return count\n",
    "\n",
    "def process_text(sentences, article_id,  max_sent_len):\n",
    "    processed = []\n",
    "    for s in sentences:\n",
    "        # ignore sentences of length 1\n",
    "        if len(s) <= 1:\n",
    "            continue\n",
    "        # ignore sentences consisting exclusively of punctuation\n",
    "        if not any([not t.is_punct for t in s]):\n",
    "            continue\n",
    "        # ignore sentences not containing any letter\n",
    "        if not any([any([c.isalpha() for c in t.text]) for t in s]):\n",
    "            continue\n",
    "        if len(s) > max_sent_len:\n",
    "            max_sent_len = len(s)\n",
    "        processed.append({\n",
    "            'article_id': article_id,\n",
    "            'org': s.text,\n",
    "            'lbl': 0.0,\n",
    "            'tokenized': [t.text for t in s],\n",
    "            'tokenized_lower': [t.text.lower() for t in s]\n",
    "        })\n",
    "    return processed, max_sent_len\n",
    "\n",
    "data = []\n",
    "max_sent_len = 0\n",
    "for article_id, article in enumerate(read_data(DATA_PATH)):\n",
    "    title = spacy_model(article['Title']).sents\n",
    "    teaser = spacy_model(article['Teaser']).sents\n",
    "    text = spacy_model(article['Text']).sents\n",
    "    \n",
    "    p_title, max_sent_len = process_text(title, article_id, max_sent_len)\n",
    "    p_teaser, max_sent_len = process_text(teaser, article_id, max_sent_len)\n",
    "    p_text, max_sent_len = process_text(text, article_id, max_sent_len)\n",
    "       \n",
    "    article_data = p_title + p_teaser + p_text\n",
    "\n",
    "    # Label sentences\n",
    "    false_statements = [article['False_Statement_1'], article['False_Statement_2'], article['False_Statement_3']]     \n",
    "    for fs in false_statements:\n",
    "        if fs != '':\n",
    "            fs_tokens = [t.text.lower() for t in spacy_model(fs)]\n",
    "            matches = [count_matches(fs_tokens, t) for t in [d['tokenized_lower'] for d in article_data]]\n",
    "            m = max(matches)\n",
    "            max_indexes = [i for i, j in enumerate(matches) if j == m]\n",
    "            \n",
    "            # +++++++ DEBUG CODE - START +++++++++ #\n",
    "            #if article_id == 400:\n",
    "            #    print(\"\\n\\nFalse Statement: {} \\n\\n\".format(fs))\n",
    "            #    for mi in max_indexes:\n",
    "            #        print(article_data[mi]['org'])\n",
    "            # +++++++ DEBUG CODE - END   +++++++++ #\n",
    "                \n",
    "            for mi in max_indexes:\n",
    "                article_data[mi]['lbl'] = 1.0\n",
    "            \n",
    "    data = data + article_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf_stats = 0\n",
    "for a in read_data(DATA_PATH):\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: not d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_deps(doc, max_sent_len):\n",
    "    oh_vectors = []\n",
    "    for token in doc:\n",
    "        vec = np.zeros(max_sent_len)\n",
    "        vec[token.head.i] = 1\n",
    "        oh_vectors.append(vec)\n",
    "        \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(oh_vectors) < max_sent_len:\n",
    "        oh_vectors.append(np.zeros(max_sent_len))\n",
    "    return oh_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    doc = spacy_model(d['org'])\n",
    "    d['processed'] = to_deps(doc, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentence, max_sent_len):\n",
    "    vectorized_sentence = []\n",
    "    vector_dim = w2v_model.wv.vector_size\n",
    "    for word in sentence:\n",
    "        if word in w2v_model.wv:\n",
    "            vectorized_sentence.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            vectorized_sentence.append(np.zeros(vector_dim))\n",
    "            \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(vectorized_sentence) < max_sent_len:\n",
    "        vectorized_sentence.append(np.zeros(vector_dim))\n",
    "        \n",
    "    return vectorized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    embedded_words = embed(d['tokenized_lower'], max_sent_len)\n",
    "    d['processed'] = np.concatenate((embedded_words, d['processed']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is seperated by article because of MAP evaluation later\n",
    "num_train_articles = int(DATASET_TRAIN_SPLIT * NUM_ARTICLES)\n",
    "train_data = list(filter(lambda d: d['article_id'] <= num_train_articles, data))\n",
    "test_data = list(filter(lambda d: d['article_id'] > num_train_articles, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_embeddings(data):\n",
    "    word_vector_dim = w2v_model.wv.vector_size\n",
    "    for d in data:\n",
    "        word_embeddings = [w[:word_vector_dim] for w in d['processed']]\n",
    "        yield np.mean(word_embeddings, axis=0)\n",
    "        \n",
    "def retrieve_topk_ixs(entry_index, data, k, sims):\n",
    "    topk_stack = [(0,0)]\n",
    "    \n",
    "    for i, sim in enumerate(sims):\n",
    "        is_greater = any([sim > tk_sim for (index, tk_sim) in topk_stack])\n",
    "        negative_label = data[entry_index]['lbl'] != data[i]['lbl']\n",
    "        not_own_sim = entry_index != i\n",
    "        \n",
    "        if is_greater and negative_label and not_own_sim: \n",
    "            if len(topk_stack) >= k:\n",
    "                topk_stack.pop()\n",
    "\n",
    "            topk_stack.append((i, sim))    \n",
    "            topk_stack.sort(reverse=True)\n",
    "    return [index for (index, sim) in topk_stack]\n",
    "\n",
    "# only use train data\n",
    "# no negative sampling for test data neccesary\n",
    "sentence_embeddings = list(compute_sentence_embeddings(train_data))\n",
    "\n",
    "similarities = cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "k = NUM_SAMPLING_CANDIDATES\n",
    "processed_topk_candidates = []\n",
    "for i, row_sims in enumerate(similarities):\n",
    "    top_k_ixs = retrieve_topk_ixs(i, data, k, row_sims)\n",
    "    \n",
    "    top_k_processed = []    \n",
    "    for top_k_ix in top_k_ixs:\n",
    "        top_k_processed.append(train_data[top_k_ix]['processed']) \n",
    "    processed_topk_candidates.append(top_k_processed)\n",
    "    \n",
    "\n",
    "def assign_candidate(d, ptc):\n",
    "    d_copy = dict(d)\n",
    "    d_copy['cs'] = ptc\n",
    "    return d_copy\n",
    "    \n",
    "train_data = [[assign_candidate(d, ptc) for ptc in ptcs] for d, ptcs in zip(train_data, processed_topk_candidates)]\n",
    "\n",
    "flatten = lambda lst: [j for sub in lst for j in sub]\n",
    "train_data = flatten(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    # yield successive n-sized chunks from lst.\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def serialize_wsampling(sdata, chunk_size, file_suffix):\n",
    "    aid_chunks = chunks([d['article_id'] for d in sdata], chunk_size)\n",
    "    X_chunks = chunks([d['processed'] for d in sdata], chunk_size)\n",
    "    y_chunks = chunks([d['lbl'] for d in sdata], chunk_size)\n",
    "    cs_chunks = chunks([d['cs'] for d in sdata], chunk_size)\n",
    "\n",
    "    zipped_chunks = zip(aid_chunks, X_chunks, y_chunks, cs_chunks)\n",
    "    for (i, (aid_chunk, X_chunk, y_chunk, cs_chunk)) in enumerate(zipped_chunks):\n",
    "        writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}_{}'.format(file_suffix, i) + '.tfrecords')\n",
    "        for (aidc, xc, yc, csc) in zip(aid_chunk, X_chunk, y_chunk, cs_chunk):\n",
    "            # Convert to TFRecords and save to file\n",
    "            feature = {\n",
    "                'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aidc])),\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc])),\n",
    "                'cs': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(csc).flatten()))\n",
    "            }\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "        writer.close()\n",
    "        \n",
    "def serialize(sdata, chunk_size, file_suffix):\n",
    "    aid_chunks = chunks([d['article_id'] for d in sdata], chunk_size)\n",
    "    X_chunks = chunks([d['processed'] for d in sdata], chunk_size)\n",
    "    y_chunks = chunks([d['lbl'] for d in sdata], chunk_size)\n",
    "    \n",
    "    zipped_chunks = zip(aid_chunks, X_chunks, y_chunks)\n",
    "    for (i, (aid_chunk, X_chunk, y_chunk)) in enumerate(zipped_chunks):\n",
    "        writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}_{}'.format(file_suffix, i) + '.tfrecords')\n",
    "        for (aidc, xc, yc) in zip(aid_chunk, X_chunk, y_chunk):\n",
    "            # Convert to TFRecords and save to file\n",
    "            feature = {\n",
    "                'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aidc])),\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc]))\n",
    "            }\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "        writer.close()\n",
    "        \n",
    "chunk_size = 2000\n",
    "serialize(train_data, chunk_size, 'TRAIN_SAMPLING')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize Base Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_wsampling(train_data, chunk_size, 'TRAIN_SAMPLING')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(train_data, chunk_size, 'TRAIN')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_parser at 0x7f26c016a050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function input_parser at 0x7f26c016a050> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function input_parser_cs at 0x7f26c016a830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function input_parser_cs at 0x7f26c016a830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f26c18688c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f26c18688c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f26c01660e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f26c01660e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f27003da440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f27003da440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n"
     ]
    }
   ],
   "source": [
    "def input_parser(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64), \n",
    "                           'x': tf.io.FixedLenFeature([135, 285], dtype=tf.float32),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    return (parsed['article_id'],parsed['x'],parsed['y'])\n",
    "\n",
    "def input_parser_cs(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64), \n",
    "                           'x': tf.io.FixedLenFeature([135, 285], dtype=tf.float32),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32),\n",
    "                           'cs': tf.io.FixedLenFeature([135, 285], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    return (parsed['article_id'],parsed['x'],parsed['y'],parsed['cs'])\n",
    "\n",
    "train_data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_TRAIN_*.tfrecords')\n",
    "train_data_raw = tf.data.TFRecordDataset(train_data_files)\n",
    "train_dataset = train_data_raw.map(input_parser)\n",
    "\n",
    "train_sampling_data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_TRAIN_SAMPLING_*.tfrecords')\n",
    "train_sampling_data_raw = tf.data.TFRecordDataset(train_sampling_data_files)\n",
    "train_sampling_dataset = train_sampling_data_raw.map(input_parser_cs)\n",
    "\n",
    "test_data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_TEST_*.tfrecords')\n",
    "test_data_raw = tf.data.TFRecordDataset(train_data_files)\n",
    "test_dataset = test_data_raw.map(input_parser)\n",
    "\n",
    "# shuffling seems to produce an error, maybe include later again\n",
    "#train_dataset = train_dataset.map(lambda ida, x, y, topk: (x, y, topk)).shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# there has already been a train/test data split in preprocessing\n",
    "train_dataset_size = int(DATASET_SIZE * DATASET_TRAIN_SPLIT)\n",
    "\n",
    "train_sampling_dataset_size = int(train_dataset_size * NUM_SAMPLING_CANDIDATES * DATASET_DEV_SPLIT)\n",
    "train_sampling_dataset = train_sampling_dataset.map(lambda ida, x, y, cs: ({'in_s1': x, 'in_s2': cs}, {'out_s1': y,'out_diff': y}))\n",
    "train_sampling_dataset_split = train_sampling_dataset.take(train_sampling_dataset_size).batch(BATCH_SIZE)\n",
    "dev_sampling_dataset = train_sampling_dataset.skip(train_sampling_dataset_size).batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset_size = int(DATASET_SIZE * DATASET_DEV_SPLIT)\n",
    "train_dataset = train_dataset.map(lambda ida, x, y: (x, y)).batch(BATCH_SIZE)\n",
    "train_dataset_split = train_dataset.take(train_dataset_size).batch(BATCH_SIZE)\n",
    "dev_dataset = train_dataset.skip(train_dataset_size).batch(BATCH_SIZE)\n",
    "\n",
    "# the eval examples do contain article_id to determine MAP\n",
    "test_dataset_eval = test_dataset\n",
    "# test examples for model don't contain article id\n",
    "test_dataset = test_dataset.map(lambda ida, x, y: (x, y)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_sampling_dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOURCE: https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'return_attention': self.return_attention,\n",
    "            'init': self.init,\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model(model_name='base'):\n",
    "    inp_shape = (135, 285)\n",
    "    model = Sequential(name=model_name)\n",
    "    model.add(LSTM(LSTM_HIDDEN_UNITS, input_shape=inp_shape, return_sequences = True, name='lstm'))\n",
    "    model.add(Attention(name='attention'))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(1, activation='sigmoid', name='dense'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ranking_model():\n",
    "    inp_shape = (135, 285)\n",
    "    \n",
    "    in_s1 = Input(inp_shape, name='in_s1')\n",
    "    in_s2 = Input(inp_shape, name='in_s2')\n",
    "    \n",
    "    base_model = build_base_model()\n",
    "    \n",
    "    out_s1 = base_model(in_s1)\n",
    "    out_s1 = Layer(name='out_s1')(tf.identity(out_s1))\n",
    "    out_s2 = base_model(in_s2)\n",
    "    out_diff = Layer(name='out_diff')(tf.math.subtract(out_s1, out_s2, name='out_diff'))\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[in_s1, in_s2], outputs=[out_s1, out_diff], name='ranking')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_callback(model_path, monotior_value):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(model_path, \n",
    "                                              monitor=monitor_value, verbose=1, \n",
    "                                              save_best_only=True, mode='max')\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_base_model()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_MAIN, 'val_binary_accuracy')\n",
    "\n",
    "history = model.fit(train_dataset_split,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "            validation_data=dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f26a882ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f26a882ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f26a882ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f26a882ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f26a882ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f26a882ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "    394/Unknown - 235s 597ms/step - loss: 0.6073 - out_s1_loss: 0.3054 - out_diff_loss: 0.9093 - out_s1_binary_accuracy: 0.9076 - out_diff_binary_accuracy: 0.9239WARNING:tensorflow:Can save best model only with val_binary_accuracy available, skipping.\n",
      "394/394 [==============================] - 296s 751ms/step - loss: 0.6073 - out_s1_loss: 0.3054 - out_diff_loss: 0.9093 - out_s1_binary_accuracy: 0.9076 - out_diff_binary_accuracy: 0.9239 - val_loss: 0.5223 - val_out_s1_loss: 0.2614 - val_out_diff_loss: 0.7831 - val_out_s1_binary_accuracy: 0.9138 - val_out_diff_binary_accuracy: 0.9324\n"
     ]
    }
   ],
   "source": [
    "def ranking_loss(y_target, y_diff):\n",
    "    pos = tf.constant([1.0 for i in range(BATCH_SIZE)])\n",
    "    neg = tf.constant([-1.0 for i in range(BATCH_SIZE)])\n",
    "    sign = tf.where(tf.equal(y_target,1.0), pos, neg)\n",
    "\n",
    "    return tf.math.maximum(0.0, 1.0 - sign * y_diff)\n",
    "    \n",
    "    \n",
    "model = build_ranking_model()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(),\n",
    "        ranking_loss,\n",
    "    ],\n",
    "    loss_weights=[0.5, 0.5],\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_RANKING, 'val_out_s1_binary_accuracy')\n",
    "\n",
    "history = model.fit(train_sampling_dataset_split,\n",
    "            epochs=1,\n",
    "            callbacks=[get_checkpoint_callback(MODEL_PATH_RANKING), tensorboard_callback],\n",
    "            validation_data=dev_sampling_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "test_model = tf.keras.models.load_model(MODEL_PATH_MAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "false_statement = \"Um die Ermordung unschuldiger Zivilisten in Russland zu üben, sucht die NATO für ihre Manöver russischsprachige Menschen.\"\n",
    "tokens = spacy_model(false_statement)\n",
    "deps = to_deps(tokens, 135)\n",
    "word_vecs = embed([t.text.lower() for t in tokens], 135)\n",
    "inp = np.concatenate((word_vecs, deps), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = test_model.predict(np.array( [inp,] ))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = list(test_dataset_eval.as_numpy_iterator())\n",
    "eval_data = [(ida[0], x, y[0]) for ida, x, y in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP metric is based on the official CLEF2019 implementation: \n",
    "# https://github.com/apepa/clef2019-factchecking-task1/blob/7d463336897ad1f870cb6a481953b94550c788a7/scorer/main.py#L52\n",
    "\n",
    "def mean_average_precision(data):\n",
    "    avg_precisions = []\n",
    "    article_ids = set([ida for ida, _, _ in data])\n",
    "    num_articles = len(article_ids)\n",
    "    \n",
    "    for id_article in article_ids:\n",
    "        article_examples = [(x,y) for ida, x, y in data if ida == id_article]\n",
    "        xs = [x for x,y in article_examples]\n",
    "        ys = [y for x,y in article_examples]\n",
    "        \n",
    "        num_positive = sum(ys)\n",
    "\n",
    "        predictions = [p[0] for p in test_model.predict(np.array(xs))]\n",
    "        ranked_indices = [i for i, v in sorted(enumerate(predictions), key=lambda tup: tup[1], reverse=True)]\n",
    "        \n",
    "        # ++++ DEBUG CODE - START +++ #\n",
    "        #hits = []\n",
    "        #for i in range(len(ranked_indices)):\n",
    "        #   if ys[ranked_indices[i]] == 1:\n",
    "        #        hits.append(1)\n",
    "        #    else:\n",
    "        #        hits.append(0)\n",
    "        #print(hits)\n",
    "        # ++++ DEBUG CODE - END   +++ #\n",
    "        \n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        for i in range(len(ranked_indices)):\n",
    "            if ys[ranked_indices[i]] == 1:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "            \n",
    "        if precisions:\n",
    "            avg_precisions.append(sum(precisions) / num_positive)\n",
    "        else:\n",
    "            avg_precisions.append(0)\n",
    "        \n",
    "    return sum(avg_precisions) / num_articles\n",
    "    \n",
    "print('MAP: {}'.format(mean_average_precision(eval_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### MAP\n",
    "- Simple Model: 0.44787412447440544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 - python",
   "language": "python",
   "name": "ipython_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
