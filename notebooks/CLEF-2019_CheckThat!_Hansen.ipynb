{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer, Dropout, LSTM, Dense, InputLayer\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "NUM_ARTICLES = 489\n",
    "MODEL_NAME = \"CLEF_2019_HANSEN\"\n",
    "MODEL_PATH_MAIN = '../models/' + MODEL_NAME\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "SEED = 12345\n",
    "LSTM_HIDDEN_UNITS = 100\n",
    "EPOCHS = 10\n",
    "CROSS_VALIDATION_K_FOLDS = 19\n",
    "DATASET_SIZE = 14765\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(MODEL_PATH_SPACY, disable=[\"vocab\"])\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Statement: So übel wird gegen europäische & deutsche Frauen gehetzt!\n",
      " \n",
      "\n",
      "\n",
      "– So übel wird gegen europäische & deutsche Frauen gehetzt!\n",
      "– So übel wird gegen europäische & deutsche Frauen gehetzt!\n",
      "„Weißer, christlicher Abschaum!“ – So übel wird gegen europäische & deutsche Frauen gehetzt!\n",
      "False Statement: TÖCHTER DER EUROPÄER SOLLEN IHRE „SEXUELLE PFLICHT“ MIT MIGRANTEN ERFÜLLEN+++DER „SEXUELLE DSCHIHAD“ WIRD PROPAGIERT+++VERUNGLIMPFUNG VON BEHINDERTEN+++\n",
      " \n",
      "\n",
      "\n",
      "3)TÖCHTER DER EUROPÄER SOLLEN IHRE\n",
      "„SEXUELLE PFLICHT“\n",
      "„SEXUELLE DSCHIHAD“\n",
      "Christinnen werden als „weißer, christlicher Abschaum“ oder als „christliche Schlampen“ bezeichnet.\n",
      "Hier: Vor allem die „Töchter“ der (weißen) Europäer geraten  in den Fokus,  die ihre „sexuelle Pflicht“ mit den Migranten erfüllen sollen.\n",
      "Hier: Denn die Männer (Migranten) aus der „Dritten Welt“ sollen nach Europa kommen, um europäische Frauen zu „beglücken“.\n",
      "Hier: Die scheinbar „arische Rasse „soll nicht von Waffengeräuschen erobert werden, sondern durch das „lustvolle“ Stöhnen einer „arischen“ Frau.\n",
      "False Statement: m Prinzip kommen als Hintermänner Rechtsextreme in Frage, die so Migranten verunglimpfen wollen. Aber auch Flüchtlingsschlepper, um noch mehr Migranten nach Europa zu locken und damit weiter große Kasse zu machen. Oder andere Interessensgruppen, die vielleicht im politischen Bereich angesiedelt sind. Jedenfalls sehen die Seiten aus, als ob sie „organisiert“ erstellt wurden.\n",
      " \n",
      "\n",
      "\n",
      "Aber auch Flüchtlingsschlepper, um noch mehr Migranten nach Europa zu locken und damit weiter große Kasse zu machen.\n"
     ]
    }
   ],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def count_matches(false_statement, sentence):\n",
    "    count = 0\n",
    "    sent_copy = sentence[:]\n",
    "    for w in false_statement:\n",
    "        if w in sent_copy:\n",
    "            count += 1\n",
    "            sent_copy.remove(w)\n",
    "    return count\n",
    "\n",
    "\n",
    "data = []\n",
    "max_sent_len = 0\n",
    "for article_id, article in enumerate(read_data(DATA_PATH)):\n",
    "    # Concatenate article text\n",
    "    text = article['Title'] + article['Teaser'] + article['Text']\n",
    "    \n",
    "    sentences = spacy_model(text).sents\n",
    "    article_data = []\n",
    "    for s in sentences:\n",
    "        if len(s) > max_sent_len:\n",
    "            max_sent_len = len(s)\n",
    "        article_data.append({\n",
    "            'article_id': article_id,\n",
    "            'org': s.text,\n",
    "            'lbl': True,\n",
    "            'tokenized': [t.text for t in s],\n",
    "            'tokenized_lower': [t.text.lower() for t in s]\n",
    "        })\n",
    " \n",
    "    # Label sentences\n",
    "    # The sentences matching the most tokens with a false statement will be labeled as False\n",
    "    false_statements = [article['False_Statement_1'], article['False_Statement_2'], article['False_Statement_3']]     \n",
    "    for fs in false_statements:\n",
    "        if fs != '':\n",
    "            fs_words = [t.text for t in spacy_model(fs)]\n",
    "            matches = [count_matches(fs_words, s) for s in [d['tokenized'] for d in article_data]]\n",
    "            m = max(matches)\n",
    "            max_indexes = [i for i, j in enumerate(matches) if j == m]\n",
    "            \n",
    "            # +++++++ DEBUG CODE - START +++++++++ #\n",
    "            if article_id == 458:\n",
    "                print(\"False Statement: {} \\n\\n\".format(fs))\n",
    "                for mi in max_indexes:\n",
    "                    print(article_data[mi]['org'])\n",
    "            # +++++++ DEBUG CODE - END +++++++++ #\n",
    "                \n",
    "            for mi in max_indexes:\n",
    "                article_data[mi]['lbl'] = False\n",
    "            \n",
    "    data = data + article_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences 14765\n",
      "True number of false statements 974\n",
      "Classified number of false statements 1022 (104.9%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in read_data(DATA_PATH):\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: not d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_deps(doc, max_sent_len):\n",
    "    oh_vectors = []\n",
    "    for token in doc:\n",
    "        vec = np.zeros(max_sent_len)\n",
    "        vec[token.head.i] = 1\n",
    "        oh_vectors.append(vec)\n",
    "        \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(oh_vectors) < max_sent_len:\n",
    "        oh_vectors.append(np.zeros(max_sent_len))\n",
    "    return oh_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    doc = spacy_model(d['org'])\n",
    "    d['processed'] = to_deps(doc, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentence, max_sent_len):\n",
    "    vectorized_sentence = []\n",
    "    vector_dim = w2v_model.wv.vector_size\n",
    "    for word in sentence:\n",
    "        if word in w2v_model.wv:\n",
    "            vectorized_sentence.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            vectorized_sentence.append(np.zeros(vector_dim))\n",
    "            \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(vectorized_sentence) < max_sent_len:\n",
    "        vectorized_sentence.append(np.zeros(vector_dim))\n",
    "        \n",
    "    return vectorized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    d['processed'] = np.concatenate((embed(d['tokenized_lower'], max_sent_len), d['processed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels are 0 for true and 1 for false statements\n",
    "y = [0.0 if d['lbl'] == True else 1.0 for d in data]\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "chunk_size = 2000\n",
    "id_chunks = chunks([d['article_id'] for d in data], chunk_size)\n",
    "X_chunks = chunks([d['processed'] for d in data], chunk_size)\n",
    "y_chunks = chunks(y, chunk_size)\n",
    "\n",
    "for (i, (id_chunk, X_chunk, y_chunk)) in enumerate(zip(id_chunks, X_chunks, y_chunks)):\n",
    "    writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}'.format(i) + '.tfrecords')\n",
    "    for (idc, xc, yc) in zip(id_chunk, X_chunk, y_chunk):\n",
    "        # Convert to TFRecords and save to file\n",
    "        feature = {\n",
    "            'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[idc])),\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_parser at 0x7f477c18f320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function input_parser at 0x7f477c18f320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f477c18fef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f477c18fef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f477c18ff80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f477c18ff80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f477d627b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f477d627b90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Str'\n"
     ]
    }
   ],
   "source": [
    "def input_parser(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64), \n",
    "                           'x': tf.io.FixedLenFeature([135, 285], dtype=tf.float32),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed_example = tf.io.parse_single_example(example, feature_description)\n",
    "    return (parsed_example['article_id'], parsed_example['x'], parsed_example['y'])\n",
    "\n",
    "data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_*.tfrecords')\n",
    "raw_dataset = tf.data.TFRecordDataset(data_files)\n",
    "dataset = raw_dataset.map(input_parser)\n",
    "\n",
    "num_train_articles = int(0.8 * NUM_ARTICLES)\n",
    "num_train_articles = tf.constant(num_train_articles, dtype=tf.int64)\n",
    "\n",
    "\n",
    "rem_article_id = lambda ida, x, y: (x, y)\n",
    "is_train_example = lambda ida, x, y: tf.squeeze(tf.math.less_equal(ida, num_train_articles))\n",
    "is_no_train_example = lambda ida, x, y: tf.squeeze(tf.math.greater(ida, num_train_articles))\n",
    "\n",
    "# model examples do not contain article_id\n",
    "train_dataset = dataset.filter(is_train_example)\n",
    "train_dataset = train_dataset.map(rem_article_id).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = dataset.filter(is_no_train_example)\n",
    "test_dataset = test_dataset.map(rem_article_id).batch(BATCH_SIZE)\n",
    "\n",
    "# the eval example do contain article_id to determine MAP\n",
    "test_dataset_eval = dataset.filter(is_no_train_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without ranking loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOURCE: https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'return_attention': self.return_attention,\n",
    "            'init': self.init,\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inp_shape = (135, 285)\n",
    "    model = Sequential(name='simple')\n",
    "    model.add(LSTM(LSTM_HIDDEN_UNITS, input_shape=inp_shape, return_sequences = True, name='lstm'))\n",
    "    model.add(Attention(name='attention'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid', name='dense'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for (fold, (train_index, val_index)) in enumerate(KFold(CROSS_VALIDATION_K_FOLDS).split(train_dataset)):\n",
    "    X_train_fold, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "    \n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH_MAIN + \"_FOLD_{}\".format(fold), \n",
    "                                                             monitor='val_accuracy', verbose=1, \n",
    "                                                             save_best_only=True, mode='max')\n",
    "    \n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(x=X_train_fold, y=y_train_fold,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "                validation_data=(X_val, y_val))\n",
    "    \n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH_MAIN, \n",
    "                                                         monitor='val_accuracy', verbose=1, \n",
    "                                                         save_best_only=True, mode='max')\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "            validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "test_model = tf.keras.models.load_model(MODEL_PATH_MAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "false_statement = \"Um die Ermordung unschuldiger Zivilisten in Russland zu üben, sucht die NATO für ihre Manöver russischsprachige Menschen.\"\n",
    "tokens = spacy_model(false_statement)\n",
    "deps = to_deps(tokens, 135)\n",
    "word_vecs = embed([t.text.lower() for t in tokens], 135)\n",
    "inp = np.concatenate((word_vecs, deps), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53837883]]\n"
     ]
    }
   ],
   "source": [
    "prediction = test_model.predict(np.array( [inp,] ))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = list(test_dataset_eval.as_numpy_iterator())\n",
    "eval_data = [(ida[0], x, y[0]) for ida, x, y in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.6367217658657178\n"
     ]
    }
   ],
   "source": [
    "# MAP metric is based on the official CLEF2019 implementation: \n",
    "# https://github.com/apepa/clef2019-factchecking-task1/blob/7d463336897ad1f870cb6a481953b94550c788a7/scorer/main.py#L52\n",
    "\n",
    "def mean_average_precision(data):\n",
    "    avg_precisions = []\n",
    "    article_ids = set([ida for ida, _, _ in data])\n",
    "    num_articles = len(article_ids)\n",
    "    \n",
    "    for id_article in article_ids:\n",
    "        article_examples = [(x,y) for ida, x, y in data if ida == id_article]\n",
    "        xs = [x for x,y in article_examples]\n",
    "        ys = [y for x,y in article_examples]\n",
    "        \n",
    "        num_positive = sum(ys)\n",
    "\n",
    "        predictions = [p[0] for p in test_model.predict(np.array(xs))]\n",
    "        ranked_indices = [i for i, v in sorted(enumerate(predictions), key=lambda tup: tup[1], reverse=True)]\n",
    "        \n",
    "        # ++++ DEBUG CODE - START +++ #\n",
    "        #hits = []\n",
    "        #for i in range(len(ranked_indices)):\n",
    "        #   if ys[ranked_indices[i]] == 1:\n",
    "        #        hits.append(1)\n",
    "        #    else:\n",
    "        #        hits.append(0)\n",
    "        #print(hits)\n",
    "        # ++++ DEBUG CODE - END   +++ #\n",
    "        \n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        for i in range(len(ranked_indices)):\n",
    "            if ys[ranked_indices[i]] == 1:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "            \n",
    "        if precisions:\n",
    "            avg_precisions.append(sum(precisions) / num_positive)\n",
    "        else:\n",
    "            avg_precisions.append(0)\n",
    "        \n",
    "    return sum(avg_precisions) / num_articles\n",
    "    \n",
    "print('MAP: {}'.format(mean_average_precision(eval_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 - python",
   "language": "python",
   "name": "ipython_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
