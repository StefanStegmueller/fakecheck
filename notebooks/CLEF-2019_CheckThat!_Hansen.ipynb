{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stefan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "nk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/GermanFakeNC.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['–', 'Mai', '2013', ':', 'Auf', 'der', 'griechischen', 'Ferieninsel', 'Korfu', 'überfällt', 'Hussein', 'Khavari', 'eine', 'Studentin', 'und', 'wirft', 'sie', 'eine', 'hohe', 'Klippe', 'hinab', '.'], True]\n"
     ]
    }
   ],
   "source": [
    "def count_matches(false_statement, sentence, count):\n",
    "    count = 0\n",
    "    sent_copy = sentence[:]\n",
    "    for w in false_statement:\n",
    "        if w in sent_copy:\n",
    "            count += 1\n",
    "            sent_copy.remove(w)\n",
    "    return count\n",
    "\n",
    "labeled_sentences = []\n",
    "for article in read_data(DATA_PATH):\n",
    "    # Concatenate article text\n",
    "    text = article['Title'] + article['Teaser'] + article['Text']\n",
    "    \n",
    "    # Seperate article text into sentences\n",
    "    sentences = sent_tokenize(text, language='german')\n",
    "    \n",
    "    # Seperate sentences into words\n",
    "    sentences = [word_tokenize(sent, language='german') for sent in sentences]\n",
    "    \n",
    "    # Label sentences\n",
    "    # The sentences matching the most tokens with a false statement will be labeled as False\n",
    "    false_statements = [article['False_Statement_1'], article['False_Statement_2'], article['False_Statement_3']]     \n",
    "    labeled_article_sentences = [[s, True] for s in sentences]\n",
    "    for fs in false_statements:\n",
    "        if fs != '':\n",
    "            fs_words = word_tokenize(fs, language='german')\n",
    "            matches = [count_matches(fs_words, s, 0) for s in sentences]\n",
    "            m = max(matches)\n",
    "            max_indexes = [i for i, j in enumerate(matches) if j == m]\n",
    "            for mi in max_indexes:\n",
    "                labeled_article_sentences[mi][1] = False\n",
    "            \n",
    "    labeled_sentences = labeled_sentences + labeled_article_sentences\n",
    "print(labeled_sentences[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all statements 11252\n",
      "True number of false statements 974\n",
      "Classified number of false statements 951 (97.6%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in read_data(DATA_PATH):\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda x: not x[1], labeled_sentences))) \n",
    "print(\"Number of all statements {}\".format(len(labeled_sentences)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 - python",
   "language": "python",
   "name": "ipython_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
