{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Layer, Dropout, LSTM, Dense, InputLayer\n",
    "import tensorflow as tf\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_NAME = \"CLEF_2019_HANSEN\"\n",
    "MODEL_PATH_MAIN = '../models/' + MODEL_NAME\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "SEED = 12345\n",
    "LSTM_HIDDEN_UNITS = 100\n",
    "EPOCHS = 10\n",
    "CROSS_VALIDATION_K_FOLDS = 19\n",
    "DATASET_SIZE = 14765\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(MODEL_PATH_SPACY, disable=[\"vocab\"])\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def count_matches(false_statement, sentence):\n",
    "    count = 0\n",
    "    sent_copy = sentence[:]\n",
    "    for w in false_statement:\n",
    "        if w in sent_copy:\n",
    "            count += 1\n",
    "            sent_copy.remove(w)\n",
    "    return count\n",
    "\n",
    "\n",
    "data = []\n",
    "max_sent_len = 0\n",
    "for article in read_data(DATA_PATH):\n",
    "    # Concatenate article text\n",
    "    text = article['Title'] + article['Teaser'] + article['Text']\n",
    "    \n",
    "    sentences = spacy_model(text).sents\n",
    "    article_data = []\n",
    "    for s in sentences:\n",
    "        if len(s) > max_sent_len:\n",
    "            max_sent_len = len(s)\n",
    "        article_data.append({\n",
    "            'org': s.text,\n",
    "            'lbl': True,\n",
    "            'tokenized': [t.text for t in s],\n",
    "            'tokenized_lower': [t.text.lower() for t in s]\n",
    "        })\n",
    " \n",
    "    # Label sentences\n",
    "    # The sentences matching the most tokens with a false statement will be labeled as False\n",
    "    false_statements = [article['False_Statement_1'], article['False_Statement_2'], article['False_Statement_3']]     \n",
    "    for fs in false_statements:\n",
    "        if fs != '':\n",
    "            fs_words = [t.text for t in spacy_model(fs)]\n",
    "            matches = [count_matches(fs_words, s) for s in [d['tokenized'] for d in article_data]]\n",
    "            m = max(matches)\n",
    "            max_indexes = [i for i, j in enumerate(matches) if j == m]\n",
    "            for mi in max_indexes:\n",
    "                article_data[mi]['lbl'] = False\n",
    "            \n",
    "    data = data + article_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all sentences 14765\n",
      "True number of false statements 974\n",
      "Classified number of false statements 1022 (104.9%)\n"
     ]
    }
   ],
   "source": [
    "tf_stats = 0\n",
    "for a in read_data(DATA_PATH):\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: not d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_deps(doc, max_sent_len):\n",
    "    oh_vectors = []\n",
    "    for token in doc:\n",
    "        vec = np.zeros(max_sent_len)\n",
    "        vec[token.head.i] = 1\n",
    "        oh_vectors.append(vec)\n",
    "        \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(oh_vectors) < max_sent_len:\n",
    "        oh_vectors.append(np.zeros(max_sent_len))\n",
    "    return oh_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    doc = spacy_model(d['org'])\n",
    "    d['processed'] = to_deps(doc, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prozess beginnt: Mord an Freiburger StudentinProzessbeginn gegen den mutmaßlichen Mörder Hussein Khavari am Dienstag, 05. September 2017STUDENTIN\n",
      "Prozess\n",
      "1\n",
      "beginnt\n",
      "1\n",
      ":\n",
      "1\n",
      "Mord\n",
      "1\n",
      "an\n",
      "3\n",
      "Freiburger\n",
      "6\n",
      "StudentinProzessbeginn\n",
      "4\n",
      "gegen\n",
      "6\n",
      "den\n",
      "10\n",
      "mutmaßlichen\n",
      "10\n",
      "Mörder\n",
      "7\n",
      "Hussein\n",
      "12\n",
      "Khavari\n",
      "10\n",
      "am\n",
      "3\n",
      "Dienstag\n",
      "13\n",
      ",\n",
      "3\n",
      "05.\n",
      "17\n",
      "September\n",
      "3\n",
      "2017STUDENTIN\n",
      "17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"de\" id=\"603e6681dffa4a46a50085997ec01e83-0\" class=\"displacy\" width=\"3025\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Prozess</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">beginnt:</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Mord</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Freiburger</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">StudentinProzessbeginn</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">gegen</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">den</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">mutmaßlichen</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">Mörder</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">Hussein</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">Khavari</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">am</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">Dienstag,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">05.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">September</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">2017STUDENTIN</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">sb</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">oa</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M380.0,441.5 L388.0,429.5 372.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-2\" stroke-width=\"2px\" d=\"M420,439.5 C420,352.0 555.0,352.0 555.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mnr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M555.0,441.5 L563.0,429.5 547.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-3\" stroke-width=\"2px\" d=\"M770,439.5 C770,352.0 905.0,352.0 905.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,441.5 L762,429.5 778,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-4\" stroke-width=\"2px\" d=\"M595,439.5 C595,264.5 910.0,264.5 910.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M910.0,441.5 L918.0,429.5 902.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-5\" stroke-width=\"2px\" d=\"M945,439.5 C945,352.0 1080.0,352.0 1080.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mnr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1080.0,441.5 L1088.0,429.5 1072.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-6\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,264.5 1610.0,264.5 1610.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,441.5 L1287,429.5 1303,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-7\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-8\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,177.0 1615.0,177.0 1615.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,441.5 L1623.0,429.5 1607.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-9\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,352.0 1955.0,352.0 1955.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pnc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,441.5 L1812,429.5 1828,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-10\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,264.5 1960.0,264.5 1960.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1960.0,441.5 L1968.0,429.5 1952.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-11\" stroke-width=\"2px\" d=\"M420,439.5 C420,89.5 2145.0,89.5 2145.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mnr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2145.0,441.5 L2153.0,429.5 2137.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-12\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,352.0 2305.0,352.0 2305.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2305.0,441.5 L2313.0,429.5 2297.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-13\" stroke-width=\"2px\" d=\"M2520,439.5 C2520,352.0 2655.0,352.0 2655.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,441.5 L2512,429.5 2528,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-14\" stroke-width=\"2px\" d=\"M420,439.5 C420,2.0 2675.0,2.0 2675.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2675.0,441.5 L2683.0,429.5 2667.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-603e6681dffa4a46a50085997ec01e83-0-15\" stroke-width=\"2px\" d=\"M2695,439.5 C2695,352.0 2830.0,352.0 2830.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-603e6681dffa4a46a50085997ec01e83-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nk</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2830.0,441.5 L2838.0,429.5 2822.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = spacy_model(data[0]['org'])\n",
    "print(doc)\n",
    "\n",
    "for t in doc:\n",
    "    print(t.text)\n",
    "    print(t.head.i)\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentence, max_sent_len):\n",
    "    vectorized_sentence = []\n",
    "    vector_dim = w2v_model.wv.vector_size\n",
    "    for word in sentence:\n",
    "        if word in w2v_model.wv:\n",
    "            vectorized_sentence.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            vectorized_sentence.append(np.zeros(vector_dim))\n",
    "            \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(vectorized_sentence) < max_sent_len:\n",
    "        vectorized_sentence.append(np.zeros(vector_dim))\n",
    "        \n",
    "    return vectorized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    d['processed'] = np.concatenate((embed(d['tokenized_lower'], max_sent_len), d['processed']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1.0 if d['lbl'] == True else 0.0 for d in data]\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "chunk_size = 2000\n",
    "X_chunks = chunks([d['processed'] for d in data], chunk_size)\n",
    "y_chunks = chunks(y, chunk_size)\n",
    "\n",
    "for (i, (X_chunk, y_chunk)) in enumerate(zip(X_chunks, y_chunks)):\n",
    "    writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}'.format(i) + '.tfrecords')\n",
    "    for (xc, yc) in zip(X_chunk, y_chunk):\n",
    "        # Convert to TFRecords and save to file\n",
    "        feature = {\n",
    "            'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "            'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc]))\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        serialized = example.SerializeToString()\n",
    "        writer.write(serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_parser at 0x7f6640953d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: AutoGraph could not transform <function input_parser at 0x7f6640953d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "def input_parser(example):\n",
    "    feature_description = {'x': tf.io.FixedLenFeature([135, 285], dtype=tf.float32),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "    parsed_example = tf.io.parse_single_example(example, feature_description)\n",
    "    return (parsed_example['x'], parsed_example['y'])\n",
    "\n",
    "data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_*.tfrecords')\n",
    "raw_dataset = tf.data.TFRecordDataset(data_files)\n",
    "dataset = raw_dataset.map(input_parser)\n",
    "\n",
    "train_size = int(0.8 * DATASET_SIZE)\n",
    "\n",
    "train_dataset = dataset.take(train_size).batch(BATCH_SIZE)\n",
    "test_dataset = dataset.skip(train_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without ranking loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOURCE: https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'return_attention': self.return_attention,\n",
    "            'init': self.init,\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inp_shape = (135, 285)\n",
    "    model = Sequential(name='simple')\n",
    "    model.add(LSTM(LSTM_HIDDEN_UNITS, input_shape=inp_shape, return_sequences = True, name='lstm'))\n",
    "    model.add(Attention(name='attention'))\n",
    "    model.add(Dense(1, activation='sigmoid', name='dense'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for (fold, (train_index, val_index)) in enumerate(skf.split(X_train, y_train)):\n",
    "    X_train_fold, X_val = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "    \n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH_MAIN + \"_FOLD_{}\".format(fold), \n",
    "                                                             monitor='val_accuracy', verbose=1, \n",
    "                                                             save_best_only=True, mode='max')\n",
    "    \n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(x=X_train_fold, y=y_train_fold,\n",
    "                epochs=EPOCHS,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "                validation_data=(X_val, y_val))\n",
    "    \n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f663e91bf50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f663e91bf50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "Model: \"simple\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 135, 100)          154400    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 100)               235       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 154,736\n",
      "Trainable params: 154,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "     99/Unknown - 39s 394ms/step - loss: 0.2996 - accuracy: 0.9242 - precision_1: 0.9281 - recall: 0.9954\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.93871, saving model to ../models/CLEF_2019_HANSEN\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f66004cb8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f66004cb8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f663e91bf50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <__main__.Attention object at 0x7f663e91bf50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /nix/store/5585rs1zvh0jlxn2jlknsawqcd7pnkhh-python3-3.7.7-env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN/assets\n",
      "99/99 [==============================] - 55s 559ms/step - loss: 0.2996 - accuracy: 0.9242 - precision_1: 0.9281 - recall: 0.9954 - val_loss: 0.2272 - val_accuracy: 0.9387 - val_precision_1: 0.9387 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2308 - accuracy: 0.9339 - precision_1: 0.9342 - recall: 0.9996\n",
      "Epoch 00002: val_accuracy improved from 0.93871 to 0.94142, saving model to ../models/CLEF_2019_HANSEN\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f6600e988c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f6600e988c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN/assets\n",
      "99/99 [==============================] - 45s 459ms/step - loss: 0.2318 - accuracy: 0.9338 - precision_1: 0.9341 - recall: 0.9996 - val_loss: 0.1999 - val_accuracy: 0.9414 - val_precision_1: 0.9414 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9323 - precision_1: 0.9323 - recall: 1.0000\n",
      "Epoch 00003: val_accuracy did not improve from 0.94142\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 0.2315 - accuracy: 0.9321 - precision_1: 0.9321 - recall: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.9329 - val_precision_1: 0.9329 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2302 - accuracy: 0.9293 - precision_1: 0.9296 - recall: 0.9996\n",
      "Epoch 00004: val_accuracy improved from 0.94142 to 0.94243, saving model to ../models/CLEF_2019_HANSEN\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f66001f9ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f66001f9ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN/assets\n",
      "99/99 [==============================] - 46s 463ms/step - loss: 0.2321 - accuracy: 0.9291 - precision_1: 0.9294 - recall: 0.9996 - val_loss: 0.1829 - val_accuracy: 0.9424 - val_precision_1: 0.9424 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9301 - precision_1: 0.9313 - recall: 0.9985\n",
      "Epoch 00005: val_accuracy did not improve from 0.94243\n",
      "99/99 [==============================] - 36s 365ms/step - loss: 0.2203 - accuracy: 0.9298 - precision_1: 0.9310 - recall: 0.9985 - val_loss: 0.2501 - val_accuracy: 0.9150 - val_precision_1: 0.9153 - val_recall: 0.9996\n",
      "Epoch 6/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2078 - accuracy: 0.9307 - precision_1: 0.9329 - recall: 0.9973\n",
      "Epoch 00006: val_accuracy did not improve from 0.94243\n",
      "99/99 [==============================] - 36s 360ms/step - loss: 0.2078 - accuracy: 0.9308 - precision_1: 0.9331 - recall: 0.9972 - val_loss: 0.2007 - val_accuracy: 0.9407 - val_precision_1: 0.9511 - val_recall: 0.9878\n",
      "Epoch 7/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.2083 - accuracy: 0.9292 - precision_1: 0.9304 - recall: 0.9984\n",
      "Epoch 00007: val_accuracy did not improve from 0.94243\n",
      "99/99 [==============================] - 35s 351ms/step - loss: 0.2073 - accuracy: 0.9294 - precision_1: 0.9306 - recall: 0.9984 - val_loss: 0.1762 - val_accuracy: 0.9407 - val_precision_1: 0.9421 - val_recall: 0.9982\n",
      "Epoch 8/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1894 - accuracy: 0.9338 - precision_1: 0.9369 - recall: 0.9960\n",
      "Epoch 00008: val_accuracy did not improve from 0.94243\n",
      "99/99 [==============================] - 35s 354ms/step - loss: 0.1905 - accuracy: 0.9337 - precision_1: 0.9367 - recall: 0.9960 - val_loss: 0.1831 - val_accuracy: 0.9404 - val_precision_1: 0.9424 - val_recall: 0.9971\n",
      "Epoch 9/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1800 - accuracy: 0.9351 - precision_1: 0.9414 - recall: 0.9919\n",
      "Epoch 00009: val_accuracy did not improve from 0.94243\n",
      "99/99 [==============================] - 36s 363ms/step - loss: 0.1803 - accuracy: 0.9352 - precision_1: 0.9414 - recall: 0.9920 - val_loss: 0.1640 - val_accuracy: 0.9380 - val_precision_1: 0.9384 - val_recall: 0.9993\n",
      "Epoch 10/10\n",
      "98/99 [============================>.] - ETA: 0s - loss: 0.1698 - accuracy: 0.9406 - precision_1: 0.9454 - recall: 0.9936\n",
      "Epoch 00010: val_accuracy improved from 0.94243 to 0.94345, saving model to ../models/CLEF_2019_HANSEN\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f65e7dfa4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x7f65e7dfa4d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN/assets\n",
      "99/99 [==============================] - 46s 466ms/step - loss: 0.1693 - accuracy: 0.9407 - precision_1: 0.9454 - recall: 0.9936 - val_loss: 0.1454 - val_accuracy: 0.9434 - val_precision_1: 0.9460 - val_recall: 0.9952\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(MODEL_PATH_MAIN, \n",
    "                                                         monitor='val_accuracy', verbose=1, \n",
    "                                                         save_best_only=True, mode='max')\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "            validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "false_statement = \"Pegida Schweiz übertrug den Trauermarsch auf Facebook live ins Internet.\"\n",
    "tokens = spacy_model(false_statement)\n",
    "deps = to_deps(tokens, 135)\n",
    "word_vecs = embed([t.text.lower() for t in tokens], 135)\n",
    "inp = np.concatenate((word_vecs, deps), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "test_model = tf.keras.models.load_model(MODEL_PATH_MAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8175335]]\n"
     ]
    }
   ],
   "source": [
    "prediction = test_model.predict(np.array( [inp,] ))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 - python",
   "language": "python",
   "name": "ipython_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
