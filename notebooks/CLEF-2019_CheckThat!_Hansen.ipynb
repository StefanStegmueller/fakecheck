{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing models**:\n",
    "- Spacy model: https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-2.3.0\n",
    "- Word2Vec: Can be trained with the **Word2Vec_10kGNAD** notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.4.1\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import json\n",
    "import itertools\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import backend as K, initializers, regularizers, constraints\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Layer, Dropout, LSTM, Dense, InputLayer\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "\n",
    "DATA_PATH = '../data/GermanFakeNC.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "NUM_ARTICLES = 489\n",
    "MODEL_NAME = \"CLEF_2019_HANSEN\"\n",
    "MODEL_PATH_BASE = '../models/' + MODEL_NAME + '_BASE'\n",
    "MODEL_PATH_RANKING = '../models/' + MODEL_NAME + '_RANKING'\n",
    "MODEL_PATH_W2V = '../models/w2v.model'\n",
    "MODEL_PATH_SPACY = '../models/de_core_news_sm-2.3.0'\n",
    "SEED = 12345\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "LSTM_HIDDEN_UNITS = 100\n",
    "EPOCHS = 10\n",
    "CROSS_VALIDATION_K_FOLDS = 19\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_TRAIN_SPLIT = 0.8\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "BATCH_SIZE = 120\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Load preprocessing models\n",
    "w2v_model = Word2Vec.load(MODEL_PATH_W2V)\n",
    "spacy_model = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def count_matches(false_statement, sentence):\n",
    "    count = 0\n",
    "    sent_copy = sentence[:]\n",
    "    for w in false_statement:\n",
    "        if w in sent_copy:\n",
    "            count += 1\n",
    "            sent_copy.remove(w)\n",
    "    return count\n",
    "\n",
    "def process_text(sentences, article_id,  max_sent_len):\n",
    "    processed = []\n",
    "    for s in sentences:\n",
    "        # ignore sentences of length 1\n",
    "        if len(s) <= 1:\n",
    "            continue\n",
    "        # ignore sentences consisting exclusively of punctuation\n",
    "        if not any([not t.is_punct for t in s]):\n",
    "            continue\n",
    "        # ignore sentences not containing any letter\n",
    "        if not any([any([c.isalpha() for c in t.text]) for t in s]):\n",
    "            continue\n",
    "        if len(s) > max_sent_len:\n",
    "            max_sent_len = len(s)\n",
    "        processed.append({\n",
    "            'article_id': article_id,\n",
    "            'org': s.text,\n",
    "            'lbl': 0.0,\n",
    "            'tokenized': [t.text for t in s],\n",
    "            'tokenized_lower': [t.text.lower() for t in s]\n",
    "        })\n",
    "    return processed, max_sent_len\n",
    "\n",
    "data = []\n",
    "max_sent_len = 0\n",
    "for article_id, article in enumerate(read_data(DATA_PATH)):\n",
    "    title = spacy_model(article['Title']).sents\n",
    "    teaser = spacy_model(article['Teaser']).sents\n",
    "    text = spacy_model(article['Text']).sents\n",
    "    \n",
    "    p_title, max_sent_len = process_text(title, article_id, max_sent_len)\n",
    "    p_teaser, max_sent_len = process_text(teaser, article_id, max_sent_len)\n",
    "    p_text, max_sent_len = process_text(text, article_id, max_sent_len)\n",
    "       \n",
    "    article_data = p_title + p_teaser + p_text\n",
    "\n",
    "    # Label sentences\n",
    "    false_statements = [article['False_Statement_1'], article['False_Statement_2'], article['False_Statement_3']]     \n",
    "    for fs in false_statements:\n",
    "        if fs != '':\n",
    "            fs_tokens = [t.text.lower() for t in spacy_model(fs)]\n",
    "            matches = [count_matches(fs_tokens, t) for t in [d['tokenized_lower'] for d in article_data]]\n",
    "            m = max(matches)\n",
    "            max_indexes = [i for i, j in enumerate(matches) if j == m]\n",
    "            \n",
    "            # +++++++ DEBUG CODE - START +++++++++ #\n",
    "            #if article_id == 400:\n",
    "            #    print(\"\\n\\nFalse Statement: {} \\n\\n\".format(fs))\n",
    "            #    for mi in max_indexes:\n",
    "            #        print(article_data[mi]['org'])\n",
    "            # +++++++ DEBUG CODE - END   +++++++++ #\n",
    "                \n",
    "            for mi in max_indexes:\n",
    "                article_data[mi]['lbl'] = 1.0\n",
    "            \n",
    "    data = data + article_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling tests\n",
    "#### Options to match fake statements to sentences\n",
    "* Test if sentence is in fake statement: matched 53.7% of false statements \n",
    "* Seperate into word tokens and test if some percetage of words is in a false statement\n",
    "* Label sentence with most matching words as false statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_stats = 0\n",
    "for a in read_data(DATA_PATH):\n",
    "    for number in ['1','2','3']:\n",
    "        if a['False_Statement_' + number] != '':\n",
    "            tf_stats += 1\n",
    "            \n",
    "cf_stats = len(list(filter(lambda d: not d['lbl'], data))) \n",
    "print(\"Number of all sentences {}\".format(len(data)))\n",
    "print(\"True number of false statements {}\".format(tf_stats))\n",
    "print(\"Classified number of false statements {} ({:.1f}%)\".format(cf_stats, (cf_stats * 100) / tf_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_deps(doc, max_sent_len):\n",
    "    oh_vectors = []\n",
    "    for token in doc:\n",
    "        vec = np.zeros(max_sent_len)\n",
    "        vec[token.head.i] = 1\n",
    "        oh_vectors.append(vec)\n",
    "        \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(oh_vectors) < max_sent_len:\n",
    "        oh_vectors.append(np.zeros(max_sent_len))\n",
    "    return oh_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    doc = spacy_model(d['org'])\n",
    "    d['processed'] = to_deps(doc, max_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentence, max_sent_len):\n",
    "    vectorized_sentence = []\n",
    "    vector_dim = w2v_model.wv.vector_size\n",
    "    for word in sentence:\n",
    "        if word in w2v_model.wv:\n",
    "            vectorized_sentence.append(w2v_model.wv[word])\n",
    "        else:\n",
    "            vectorized_sentence.append(np.zeros(vector_dim))\n",
    "            \n",
    "    # padding with 0 vectors to max sentence length\n",
    "    while len(vectorized_sentence) < max_sent_len:\n",
    "        vectorized_sentence.append(np.zeros(vector_dim))\n",
    "        \n",
    "    return vectorized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    embedded_words = embed(d['tokenized_lower'], max_sent_len)\n",
    "    d['processed'] = np.concatenate((embedded_words, d['processed']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is seperated by article because of MAP evaluation later\n",
    "num_train_articles = int(DATASET_TRAIN_SPLIT * NUM_ARTICLES)\n",
    "train_data = list(filter(lambda d: d['article_id'] <= num_train_articles, data))\n",
    "test_data = list(filter(lambda d: d['article_id'] > num_train_articles, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_embeddings(data):\n",
    "    word_vector_dim = w2v_model.wv.vector_size\n",
    "    for d in data:\n",
    "        word_embeddings = [w[:word_vector_dim] for w in d['processed']]\n",
    "        yield np.mean(word_embeddings, axis=0)\n",
    "        \n",
    "def retrieve_topk_ixs(entry_index, data, k, sims):\n",
    "    topk_stack = [(0,0)]\n",
    "    \n",
    "    for i, sim in enumerate(sims):\n",
    "        is_greater = any([sim > tk_sim for (index, tk_sim) in topk_stack])\n",
    "        negative_label = data[entry_index]['lbl'] != data[i]['lbl']\n",
    "        not_own_sim = entry_index != i\n",
    "        \n",
    "        if is_greater and negative_label and not_own_sim: \n",
    "            if len(topk_stack) >= k:\n",
    "                topk_stack.pop()\n",
    "\n",
    "            topk_stack.append((i, sim))    \n",
    "            topk_stack.sort(reverse=True)\n",
    "    return [index for (index, sim) in topk_stack]\n",
    "\n",
    "# only use train data\n",
    "# no negative sampling for test data neccesary\n",
    "sentence_embeddings = list(compute_sentence_embeddings(train_data))\n",
    "\n",
    "similarities = cosine_similarity(sentence_embeddings, sentence_embeddings)\n",
    "\n",
    "k = NUM_SAMPLING_CANDIDATES\n",
    "processed_topk_candidates = []\n",
    "for i, row_sims in enumerate(similarities):\n",
    "    top_k_ixs = retrieve_topk_ixs(i, data, k, row_sims)\n",
    "    \n",
    "    top_k_processed = []    \n",
    "    for top_k_ix in top_k_ixs:\n",
    "        top_k_processed.append(train_data[top_k_ix]['processed']) \n",
    "    processed_topk_candidates.append(top_k_processed)\n",
    "    \n",
    "\n",
    "def assign_candidate(d, ptc):\n",
    "    d_copy = dict(d)\n",
    "    d_copy['cs'] = ptc\n",
    "    return d_copy\n",
    "    \n",
    "train_data = [[assign_candidate(d, ptc) for ptc in ptcs] for d, ptcs in zip(train_data, processed_topk_candidates)]\n",
    "\n",
    "flatten = lambda lst: [j for sub in lst for j in sub]\n",
    "train_data = flatten(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    # yield successive n-sized chunks from lst.\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "def serialize_wsampling(sdata, chunk_size, file_suffix):\n",
    "    aid_chunks = chunks([d['article_id'] for d in sdata], chunk_size)\n",
    "    X_chunks = chunks([d['processed'] for d in sdata], chunk_size)\n",
    "    y_chunks = chunks([d['lbl'] for d in sdata], chunk_size)\n",
    "    cs_chunks = chunks([d['cs'] for d in sdata], chunk_size)\n",
    "\n",
    "    zipped_chunks = zip(aid_chunks, X_chunks, y_chunks, cs_chunks)\n",
    "    for (i, (aid_chunk, X_chunk, y_chunk, cs_chunk)) in enumerate(zipped_chunks):\n",
    "        writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}_{}'.format(file_suffix, i) + '.tfrecords')\n",
    "        for (aidc, xc, yc, csc) in zip(aid_chunk, X_chunk, y_chunk, cs_chunk):\n",
    "            # Convert to TFRecords and save to file\n",
    "            feature = {\n",
    "                'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aidc])),\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc])),\n",
    "                'cs': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(csc).flatten()))\n",
    "            }\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "        writer.close()\n",
    "        \n",
    "def serialize(sdata, chunk_size, file_suffix):\n",
    "    aid_chunks = chunks([d['article_id'] for d in sdata], chunk_size)\n",
    "    X_chunks = chunks([d['processed'] for d in sdata], chunk_size)\n",
    "    y_chunks = chunks([d['lbl'] for d in sdata], chunk_size)\n",
    "    \n",
    "    zipped_chunks = zip(aid_chunks, X_chunks, y_chunks)\n",
    "    for (i, (aid_chunk, X_chunk, y_chunk)) in enumerate(zipped_chunks):\n",
    "        writer = tf.io.TFRecordWriter(DATA_PATH_PROCESSED + '_{}_{}'.format(file_suffix, i) + '.tfrecords')\n",
    "        for (aidc, xc, yc) in zip(aid_chunk, X_chunk, y_chunk):\n",
    "            # Convert to TFRecords and save to file\n",
    "            feature = {\n",
    "                'article_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[aidc])),\n",
    "                'x': tf.train.Feature(float_list=tf.train.FloatList(value=np.stack(xc).flatten())),\n",
    "                'y': tf.train.Feature(float_list=tf.train.FloatList(value=[yc]))\n",
    "            }\n",
    "            \n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "            serialized = example.SerializeToString()\n",
    "            writer.write(serialized)\n",
    "        writer.close()\n",
    "        \n",
    "chunk_size = 2000\n",
    "serialize(train_data, chunk_size, 'TRAIN_SAMPLING')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize Base Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_wsampling(train_data, chunk_size, 'TRAIN_SAMPLING')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize Ranking Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize(train_data, chunk_size, 'TRAIN')\n",
    "serialize(test_data, chunk_size, 'TEST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_parser(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64), \n",
    "                           'x': tf.io.FixedLenFeature([135, 285], dtype=tf.float32),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    return (parsed['article_id'],parsed['x'],parsed['y'])\n",
    "\n",
    "def input_parser_cs(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64), \n",
    "                           'x': tf.io.FixedLenFeature([135, 285], dtype=tf.float32),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32),\n",
    "                           'cs': tf.io.FixedLenFeature([135, 285], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    return (parsed['article_id'],parsed['x'],parsed['y'],parsed['cs'])\n",
    "\n",
    "train_data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_TRAIN_*.tfrecords')\n",
    "train_data_raw = tf.data.TFRecordDataset(train_data_files)\n",
    "train_dataset = train_data_raw.map(input_parser)\n",
    "\n",
    "train_sampling_data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_TRAIN_SAMPLING_*.tfrecords')\n",
    "train_sampling_data_raw = tf.data.TFRecordDataset(train_sampling_data_files)\n",
    "train_sampling_dataset = train_sampling_data_raw.map(input_parser_cs)\n",
    "\n",
    "test_data_files = tf.data.Dataset.list_files(DATA_PATH_PROCESSED + '_TEST_*.tfrecords')\n",
    "test_data_raw = tf.data.TFRecordDataset(train_data_files)\n",
    "test_dataset = test_data_raw.map(input_parser)\n",
    "\n",
    "# shuffling seems to produce an error, maybe include later again\n",
    "#train_dataset = train_dataset.map(lambda ida, x, y, topk: (x, y, topk)).shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# there has already been a train/test data split in preprocessing\n",
    "train_dataset_size = int(DATASET_SIZE * DATASET_TRAIN_SPLIT)\n",
    "\n",
    "train_sampling_dataset_size = int(train_dataset_size * NUM_SAMPLING_CANDIDATES * DATASET_DEV_SPLIT)\n",
    "train_sampling_dataset = train_sampling_dataset.map(lambda ida, x, y, cs: ({'in_s1': x, 'in_s2': cs}, {'out_s1': y,'out_diff': y}))\n",
    "train_sampling_dataset_split = train_sampling_dataset.take(train_sampling_dataset_size).batch(BATCH_SIZE)\n",
    "dev_sampling_dataset = train_sampling_dataset.skip(train_sampling_dataset_size).batch(BATCH_SIZE)\n",
    "\n",
    "train_dataset_size = int(DATASET_SIZE * DATASET_DEV_SPLIT)\n",
    "train_dataset = train_dataset.map(lambda ida, x, y: (x, y))\n",
    "train_dataset_split = train_dataset.take(train_dataset_size).batch(BATCH_SIZE)\n",
    "dev_dataset = train_dataset.skip(train_dataset_size).batch(BATCH_SIZE)\n",
    "\n",
    "# the eval examples do contain article_id to determine MAP\n",
    "test_dataset_eval = test_dataset\n",
    "# test examples for model don't contain article id\n",
    "test_dataset = test_dataset.map(lambda ida, x, y: (x, y)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(135, 285), dtype=float32, numpy=\n",
      "array([[ 2.0907426 ,  0.22835703,  1.4968257 , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.02685186,  0.40970182, -0.16604422, ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [-2.2532084 ,  2.248028  ,  3.177055  , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       ...,\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOURCE: https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'return_attention': self.return_attention,\n",
    "            'init': self.init,\n",
    "            'W_regularizer': self.W_regularizer,\n",
    "            'b_regularizer': self.b_regularizer,\n",
    "            'W_constraint': self.W_constraint,\n",
    "            'b_constraint': self.b_constraint,\n",
    "            'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_model(model_name='base'):\n",
    "    inp_shape = (135, 285)\n",
    "    model = Sequential(name=model_name)\n",
    "    model.add(LSTM(LSTM_HIDDEN_UNITS, input_shape=inp_shape, return_sequences = True, name='lstm'))\n",
    "    model.add(Attention(name='attention'))\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    model.add(Dense(1, activation='sigmoid', name='dense'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ranking_model():\n",
    "    inp_shape = (135, 285)\n",
    "    \n",
    "    in_s1 = Input(inp_shape, name='in_s1')\n",
    "    in_s2 = Input(inp_shape, name='in_s2')\n",
    "    \n",
    "    base_model = build_base_model()\n",
    "    \n",
    "    out_s1 = base_model(in_s1)\n",
    "    out_s1 = Layer(name='out_s1')(tf.identity(out_s1))\n",
    "    out_s2 = base_model(in_s2)\n",
    "    out_diff = Layer(name='out_diff')(tf.math.subtract(out_s1, out_s2, name='out_diff'))\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[in_s1, in_s2], outputs=[out_s1, out_diff], name='ranking')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_callback(model_path, monitor_value):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(model_path, \n",
    "                                              monitor=monitor_value, verbose=1, \n",
    "                                              save_best_only=True, mode='max')\n",
    "\n",
    "log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"base\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 135, 100)          154400    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 100)               235       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 154,736\n",
      "Trainable params: 154,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "99/99 [==============================] - 146s 1s/step - loss: 0.3829 - binary_accuracy: 0.8996 - precision_1: 0.0547 - recall_1: 0.0290 - val_loss: 0.2609 - val_binary_accuracy: 0.9256 - val_precision_1: 0.7826 - val_recall_1: 0.0044\n",
      "\n",
      "Epoch 00001: val_binary_accuracy improved from -inf to 0.92561, saving model to ../models/CLEF_2019_HANSEN_BASE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_BASE/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_BASE/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "99/99 [==============================] - 150s 2s/step - loss: 0.3085 - binary_accuracy: 0.9093 - precision_1: 0.3118 - recall_1: 0.0108 - val_loss: 0.2570 - val_binary_accuracy: 0.9239 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "\n",
      "Epoch 00002: val_binary_accuracy did not improve from 0.92561\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 148s 2s/step - loss: 0.2500 - binary_accuracy: 0.9297 - precision_1: 0.0000e+00 - recall_1: 0.0000e+00 - val_loss: 0.2432 - val_binary_accuracy: 0.9271 - val_precision_1: 0.0000e+00 - val_recall_1: 0.0000e+00\n",
      "\n",
      "Epoch 00003: val_binary_accuracy improved from 0.92561 to 0.92707, saving model to ../models/CLEF_2019_HANSEN_BASE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_BASE/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_BASE/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "99/99 [==============================] - 139s 1s/step - loss: 0.2617 - binary_accuracy: 0.9227 - precision_1: 0.3173 - recall_1: 0.0014 - val_loss: 0.2895 - val_binary_accuracy: 0.9321 - val_precision_1: 0.8667 - val_recall_1: 0.0103\n",
      "\n",
      "Epoch 00004: val_binary_accuracy improved from 0.92707 to 0.93209, saving model to ../models/CLEF_2019_HANSEN_BASE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_fn, lstm_cell_5_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_BASE/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_BASE/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "99/99 [==============================] - 138s 1s/step - loss: 0.2467 - binary_accuracy: 0.9321 - precision_1: 0.2200 - recall_1: 7.4228e-04 - val_loss: 0.2451 - val_binary_accuracy: 0.9251 - val_precision_1: 0.8000 - val_recall_1: 0.0283\n",
      "\n",
      "Epoch 00005: val_binary_accuracy did not improve from 0.93209\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 142s 1s/step - loss: 0.2148 - binary_accuracy: 0.9393 - precision_1: 0.8033 - recall_1: 0.0697 - val_loss: 0.2420 - val_binary_accuracy: 0.9251 - val_precision_1: 1.0000 - val_recall_1: 0.0060\n",
      "\n",
      "Epoch 00006: val_binary_accuracy did not improve from 0.93209\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 133s 1s/step - loss: 0.2148 - binary_accuracy: 0.9350 - precision_1: 0.5678 - recall_1: 0.0335 - val_loss: 0.2325 - val_binary_accuracy: 0.9289 - val_precision_1: 0.8023 - val_recall_1: 0.0351\n",
      "\n",
      "Epoch 00007: val_binary_accuracy did not improve from 0.93209\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 134s 1s/step - loss: 0.2681 - binary_accuracy: 0.9147 - precision_1: 0.4243 - recall_1: 0.0169 - val_loss: 0.2275 - val_binary_accuracy: 0.9285 - val_precision_1: 0.6659 - val_recall_1: 0.0704\n",
      "\n",
      "Epoch 00008: val_binary_accuracy did not improve from 0.93209\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 135s 1s/step - loss: 0.2326 - binary_accuracy: 0.9241 - precision_1: 0.6315 - recall_1: 0.0979 - val_loss: 0.2362 - val_binary_accuracy: 0.9194 - val_precision_1: 0.3751 - val_recall_1: 0.1265\n",
      "\n",
      "Epoch 00009: val_binary_accuracy did not improve from 0.93209\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 154s 2s/step - loss: 0.2046 - binary_accuracy: 0.9370 - precision_1: 0.5126 - recall_1: 0.1278 - val_loss: 0.2432 - val_binary_accuracy: 0.9247 - val_precision_1: 1.0000 - val_recall_1: 0.0014\n",
      "\n",
      "Epoch 00010: val_binary_accuracy did not improve from 0.93209\n"
     ]
    }
   ],
   "source": [
    "model = build_base_model()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BASE, 'val_binary_accuracy')\n",
    "\n",
    "history = model.fit(train_dataset_split,\n",
    "            epochs=EPOCHS,\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "            validation_data=dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "Epoch 1/2\n",
      "394/394 [==============================] - 314s 787ms/step - loss: 0.6340 - out_s1_loss: 0.3426 - out_diff_loss: 0.9254 - out_s1_binary_accuracy: 0.9017 - out_diff_binary_accuracy: 0.9197 - val_loss: 0.3979 - val_out_s1_loss: 0.2613 - val_out_diff_loss: 0.5345 - val_out_s1_binary_accuracy: 0.9114 - val_out_diff_binary_accuracy: 0.9335\n",
      "\n",
      "Epoch 00001: val_out_s1_binary_accuracy improved from -inf to 0.91136, saving model to ../models/CLEF_2019_HANSEN_RANKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_RANKING/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_RANKING/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n",
      "394/394 [==============================] - 299s 758ms/step - loss: 0.4422 - out_s1_loss: 0.3073 - out_diff_loss: 0.5771 - out_s1_binary_accuracy: 0.8992 - out_diff_binary_accuracy: 0.9296 - val_loss: 0.3349 - val_out_s1_loss: 0.2200 - val_out_diff_loss: 0.4498 - val_out_s1_binary_accuracy: 0.9335 - val_out_diff_binary_accuracy: 0.9421\n",
      "\n",
      "Epoch 00002: val_out_s1_binary_accuracy improved from 0.91136 to 0.93352, saving model to ../models/CLEF_2019_HANSEN_RANKING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_RANKING/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/CLEF_2019_HANSEN_RANKING/assets\n"
     ]
    }
   ],
   "source": [
    "class RankingError(Loss):    \n",
    "    def call(self, y_true, y_diff):\n",
    "        pos = tf.constant([1.0 for i in range(BATCH_SIZE)])\n",
    "        neg = tf.constant([-1.0 for i in range(BATCH_SIZE)])\n",
    "        sign = tf.where(tf.equal(y_true,1.0), pos, neg)\n",
    "\n",
    "        return tf.math.maximum(0.0, 1.0 - sign * y_diff)\n",
    "    \n",
    "    \n",
    "model = build_ranking_model()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=[\n",
    "        tf.keras.losses.BinaryCrossentropy(),\n",
    "        RankingError(),\n",
    "    ],\n",
    "    loss_weights=[0.5, 0.5],\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_RANKING, 'val_out_s1_binary_accuracy')\n",
    "\n",
    "history = model.fit(train_sampling_dataset_split,\n",
    "            epochs=2,\n",
    "            callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "            validation_data=dev_sampling_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "test_model_base = tf.keras.models.load_model(MODEL_PATH_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample prediction for base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "false_statement = \"Um die Ermordung unschuldiger Zivilisten in Russland zu üben, sucht die NATO für ihre Manöver russischsprachige Menschen.\"\n",
    "tokens = spacy_model(false_statement)\n",
    "deps = to_deps(tokens, 135)\n",
    "word_vecs = embed([t.text.lower() for t in tokens], 135)\n",
    "inp = np.concatenate((word_vecs, deps), axis=1)\n",
    "print(len(inp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3697008]]\n"
     ]
    }
   ],
   "source": [
    "prediction = test_model_base.predict(np.array( [inp,] ))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ranking model\n",
    "test_model_ranking = tf.keras.models.load_model(MODEL_PATH_RANKING, compile=False)\n",
    "test_model_ranking = test_model_ranking.get_layer(name='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08147946]]\n"
     ]
    }
   ],
   "source": [
    "prediction = test_model_ranking.predict(np.array( [inp,] ))\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = list(test_dataset_eval.as_numpy_iterator())\n",
    "eval_data = [(ida[0], x, y[0]) for ida, x, y in eval_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.35671007297978213\n"
     ]
    }
   ],
   "source": [
    "# MAP metric is based on the official CLEF2019 implementation: \n",
    "# https://github.com/apepa/clef2019-factchecking-task1/blob/7d463336897ad1f870cb6a481953b94550c788a7/scorer/main.py#L52\n",
    "\n",
    "def mean_average_precision(data, model):\n",
    "    avg_precisions = []\n",
    "    article_ids = set([ida for ida, _, _ in data])\n",
    "    num_articles = len(article_ids)\n",
    "    \n",
    "    for id_article in article_ids:\n",
    "        article_examples = [(x,y) for ida, x, y in data if ida == id_article]\n",
    "        xs = [x for x,y in article_examples]\n",
    "        ys = [y for x,y in article_examples]\n",
    "        \n",
    "        num_positive = sum(ys)\n",
    "\n",
    "        predictions = [p[0] for p in model.predict(np.array(xs))]\n",
    "        ranked_indices = [i for i, v in sorted(enumerate(predictions), key=lambda tup: tup[1], reverse=True)]\n",
    "        \n",
    "        # ++++ DEBUG CODE - START +++ #\n",
    "        #hits = []\n",
    "        #for i in range(len(ranked_indices)):\n",
    "        #   if ys[ranked_indices[i]] == 1:\n",
    "        #        hits.append(1)\n",
    "        #    else:\n",
    "        #        hits.append(0)\n",
    "        #print(hits)\n",
    "        # ++++ DEBUG CODE - END   +++ #\n",
    "        \n",
    "        precisions = []\n",
    "        num_correct = 0\n",
    "        for i in range(len(ranked_indices)):\n",
    "            if ys[ranked_indices[i]] == 1:\n",
    "                num_correct += 1\n",
    "                precisions.append(num_correct / (i + 1))\n",
    "            \n",
    "        if precisions:\n",
    "            avg_precisions.append(sum(precisions) / num_positive)\n",
    "        else:\n",
    "            avg_precisions.append(0)\n",
    "        \n",
    "    return sum(avg_precisions) / num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base/MAP: 0.35671007297978213\n"
     ]
    }
   ],
   "source": [
    "print('Base/MAP: {}'.format(mean_average_precision(eval_data, test_model_base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking/MAP: 0.4634679725612787\n"
     ]
    }
   ],
   "source": [
    "print('Ranking/MAP: {}'.format(mean_average_precision(eval_data, test_model_ranking)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "|     | Base | Ranking |\n",
    "|-----|------|---------|\n",
    "| MAP |   0.35671007297978213   |  0.4634679725612787       |\n",
    "| P@1 |      |         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
