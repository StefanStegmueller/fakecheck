{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confident-least",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from evaluation import mean_average_precision, precision_at_k\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "    \n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "MODEL_PATH_BERT_TUNED = '../models/bert-base-german-cased-tuned/checkpoint.ckpt'\n",
    "MODEL_PATH_BERT_TUNED_RANKING = '../models/bert-base-german-cased-tuned-ranking/checkpoint.ckpt'\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 134\n",
    "LEARNING_RATE = 5e-5\n",
    "BINACC_THRESHOLD = 0.1\n",
    "PRECISION_RECALL_THRESHOLDS = [0.05, 0.1, 0.2, 0.5]\n",
    "EPOCHS = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excess-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "            return json.load(json_file)\n",
    "        \n",
    "def encode(sentences):\n",
    "    return tokenizer(sentences, max_length=MAX_LEN, truncation=True, padding=True, return_tensors='tf')\n",
    "        \n",
    "def to_dataset(data):\n",
    "    sentences = [d['org'] for d in data]\n",
    "    encodings_ds = tf.data.Dataset.from_tensor_slices(encode(sentences))    \n",
    "    encodings_ds = encodings_ds.map(lambda ex: {i:ex[i] for i in ex}) # Batch encoding to dictionary\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices([d['lbl'] for d in data]).map(lambda lbl: tf.reshape(lbl, [1]))\n",
    "    ids_ds = tf.data.Dataset.from_tensor_slices([d['article_id'] for d in data])\n",
    "    return tf.data.Dataset.zip((ids_ds, encodings_ds, labels_ds))\n",
    "        \n",
    "train_data = read_data(DATA_PATH_FORMATED_TRAIN)\n",
    "test_data = read_data(DATA_PATH_FORMATED_TEST)\n",
    "    \n",
    "train_ds = to_dataset(train_data).map(lambda ida, inp, lbl: (inp, lbl))\n",
    "test_ds = to_dataset(test_data).map(lambda ida, inp, lbl: (ida, inp, lbl[0]))\n",
    "\n",
    "num_train_examples = int(len(train_data) * DATASET_DEV_SPLIT)\n",
    "train_ds_split = train_ds.take(num_train_examples)\n",
    "train_ds_split = train_ds_split.shuffle(100, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "dev_ds_split = train_ds.skip(num_train_examples).batch(BATCH_SIZE)\n",
    "\n",
    "train_sampling_dataset_size = int(train_dataset_size * NUM_SAMPLING_CANDIDATES * DATASET_DEV_SPLIT)\n",
    "train_sampling_dataset = train_sampling_dataset.map(lambda x, y, cs: ({'in_s1': x, 'in_s2': cs}, {'out_s1': y,'out_diff': y}))\n",
    "train_sampling_dataset_split = train_sampling_dataset.take(train_sampling_dataset_size).batch(BATCH_SIZE)\n",
    "dev_sampling_dataset = train_sampling_dataset.skip(train_sampling_dataset_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unavailable-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_parser_train(example):\n",
    "    feature_description = {'input_ids': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'token_type_ids': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'attention_mask': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids': parsed['input_ids'],\n",
    "           'token_type_ids': parsed['token_type_ids'],\n",
    "           'attention_mask': parsed['attention_mask']}\n",
    "    return (inp, parsed['y'])\n",
    "\n",
    "def input_parser_test(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "                           'input_ids': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'token_type_ids': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'attention_mask': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids': parsed['input_ids'],\n",
    "           'token_type_ids': parsed['token_type_ids'],\n",
    "           'attention_mask': parsed['attention_mask']}\n",
    "    return (parsed['article_id'], inp, parsed['y'])\n",
    "\n",
    "def input_parser_cs(example):\n",
    "    feature_description = {'input_ids1': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'token_type_ids1': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'attention_mask1': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'input_ids2': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'token_type_ids2': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'attention_mask2': tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64),\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids1': parsed['input_ids1'],\n",
    "           'token_type_ids1': parsed['token_type_ids1'],\n",
    "           'attention_mask1': parsed['attention_mask1'],\n",
    "           'input_ids2': parsed['input_ids2'],\n",
    "           'token_type_ids2': parsed['token_type_ids2'],\n",
    "           'attention_mask2': parsed['attention_mask2']}\n",
    "    return (inp, parsed['y'])\n",
    "\n",
    "train_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TRAIN_BERT', input_parser_train)\n",
    "train_sampling_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TRAIN_BERT_SAMPLING', input_parser_cs)\n",
    "test_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TEST_BERT', input_parser_test)\n",
    "test_dataset = test_dataset.map(lambda ida, x, y: (ida[0], x, y[0]))\n",
    "\n",
    "num_train_examples = int(DATASET_SIZE * DATASET_DEV_SPLIT)\n",
    "train_ds_split = train_dataset.take(num_train_examples)\n",
    "train_ds_split = train_ds_split.shuffle(100, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "dev_ds_split = train_dataset.skip(num_train_examples).batch(BATCH_SIZE)\n",
    "\n",
    "train_sampling_dataset_size = int(DATASET_SIZE* NUM_SAMPLING_CANDIDATES * DATASET_DEV_SPLIT)\n",
    "train_sampling_dataset = train_sampling_dataset.map(lambda inp, y: (inp, {'out_s1': y,'out_diff': y}))\n",
    "train_sampling_dataset_split = train_sampling_dataset.take(train_sampling_dataset_size).batch(BATCH_SIZE)\n",
    "dev_sampling_dataset = train_sampling_dataset.skip(train_sampling_dataset_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-welding",
   "metadata": {},
   "source": [
    "### Load initial pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "plain-ability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../models/bert-base-german-cased/ were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../models/bert-base-german-cased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-japan",
   "metadata": {},
   "source": [
    "### Load fine-tuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "short-gravity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1cc83c32e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbert_model.load_weights(MODEL_PATH_BERT_TUNED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-deposit",
   "metadata": {},
   "source": [
    "### Load ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "included-european",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../models/bert-base-german-cased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f145455d580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f145455d580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "\n",
    "def cbert_model_forward(inp):\n",
    "    outputs = cbert_model(inp)\n",
    "    return outputs.logits\n",
    "\n",
    "shape=(MAX_LEN,)\n",
    "input_type=tf.int32\n",
    "\n",
    "input_ids1 = Input(shape=shape, name='input_ids1', dtype=input_type)\n",
    "attention_mask1 = Input(shape=shape, name='attention_mask1', dtype=input_type)\n",
    "token_type_ids1 = Input(shape=shape, name='token_type_ids1', dtype=input_type)\n",
    "\n",
    "input_ids2 = Input(shape=shape, name='input_ids2', dtype=input_type)\n",
    "attention_mask2 = Input(shape=shape, name='attention_mask2', dtype=input_type)\n",
    "token_type_ids2 = Input(shape=shape, name='token_type_ids2', dtype=input_type) \n",
    "\n",
    "cbert_model_ranking = build_ranking_model(cbert_model_forward,\n",
    "                                          [input_ids1, attention_mask1, token_type_ids1],\n",
    "                                          [input_ids2, attention_mask2, token_type_ids2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "copyrighted-monitor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ranking\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids1 (InputLayer)         [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask1 (InputLayer)    [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids1 (InputLayer)    [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_for_sequence_classifica TFSequenceClassifier 109082113   input_ids1[0][0]                 \n",
      "                                                                 attention_mask1[0][0]            \n",
      "                                                                 token_type_ids1[0][0]            \n",
      "                                                                 input_ids2[0][0]                 \n",
      "                                                                 attention_mask2[0][0]            \n",
      "                                                                 token_type_ids2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids2 (InputLayer)         [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask2 (InputLayer)    [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids2 (InputLayer)    [(None, 134)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "out_s1 (Layer)                  (None, 1)            0           tf_bert_for_sequence_classificati\n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 1)            0           out_s1[0][0]                     \n",
      "                                                                 tf_bert_for_sequence_classificati\n",
      "__________________________________________________________________________________________________\n",
      "out_diff (Layer)                (None, 1)            0           tf.math.subtract[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 109,082,113\n",
      "Trainable params: 109,082,113\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbert_model_ranking.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-renewal",
   "metadata": {},
   "source": [
    "### Training without ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "precision = tf.keras.metrics.Precision(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "recall = tf.keras.metrics.Recall(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "binacc = tf.keras.metrics.BinaryAccuracy(threshold=BINACC_THRESHOLD)\n",
    "metrics = [precision, recall, binacc]\n",
    "cbert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model.fit(train_ds_split,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=dev_ds_split,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-nicholas",
   "metadata": {},
   "source": [
    "### Training with ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-lunch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-cb1f2b1e5e4da925\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-cb1f2b1e5e4da925\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "    242/Unknown - 27475s 113s/step - loss: 0.7863 - out_s1_loss: 0.2880 - out_diff_loss: 0.4983 - out_s1_precision_2: 0.1510 - out_s1_recall_2: 0.5038 - out_s1_binary_accuracy: 0.4008"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "precision = tf.keras.metrics.Precision(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "recall = tf.keras.metrics.Recall(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "binacc = tf.keras.metrics.BinaryAccuracy(threshold=BINACC_THRESHOLD)\n",
    "metrics = {'out_s1': [precision, recall, binacc]}\n",
    "cbert_model_ranking.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED_RANKING, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model_ranking.fit(train_sampling_dataset_split,\n",
    "                epochs=2,\n",
    "                validation_data=dev_sampling_dataset,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pharmaceutical-governor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1cc83d3580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1cc83d3580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "BERT/MAP: 0.47965098263440786\n",
      "Ranking/P@1: 0.42857142857142855\n",
      "Ranking/P@5: 0.18571428571428555\n",
      "Ranking/P@10: 0.14285714285714268\n"
     ]
    }
   ],
   "source": [
    "def prediction_func(inps):\n",
    "    outputs = cbert_model.predict(inps)\n",
    "    return [l[0] for l in outputs.logits]\n",
    "\n",
    "eval_data_bert = batch_predict(test_ds, 100, prediction_func)\n",
    "print('BERT/MAP: {}'.format(mean_average_precision(eval_data_bert)))\n",
    "for k in [1, 5, 10]:\n",
    "    print('BERT/P@{}: {}'.format(k, precision_at_k(eval_data_bert, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-ethnic",
   "metadata": {},
   "source": [
    "### Results\n",
    "|     | BERT 2 Epochs| BERT 5 Epochs |\n",
    "|-----|------|---------|\n",
    "| MAP |   0.45336887554833294   |    0.47965098263440786    |\n",
    "| P@1 |      |     0.42857142857142855    |\n",
    "| P@5 |      |     0.18571428571428555    |\n",
    "| P@10 |      |     0.14285714285714268    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
