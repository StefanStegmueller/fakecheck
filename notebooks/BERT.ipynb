{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "given-fluid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from evaluation import mean_average_precision, precision_at_k\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "    \n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "MODEL_PATH_BERT_TUNED = '../models/bert-base-german-cased-tuned/checkpoint.ckpt'\n",
    "MODEL_PATH_BERT_TUNED_RANKING = '../models/bert-base-german-cased-tuned-ranking/checkpoint.ckpt'\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 134\n",
    "LEARNING_RATE = 5e-5\n",
    "BINACC_THRESHOLD = 0.1\n",
    "PRECISION_RECALL_THRESHOLDS = [0.05, 0.1, 0.2, 0.5]\n",
    "EPOCHS = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "prostate-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_feature = tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64)\n",
    "\n",
    "def input_parser_train(example):\n",
    "    feature_description = {'input_ids': bert_feature,\n",
    "                           'token_type_ids': bert_feature,\n",
    "                           'attention_mask': bert_feature,\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids': parsed['input_ids'],\n",
    "           'token_type_ids': parsed['token_type_ids'],\n",
    "           'attention_mask': parsed['attention_mask'],}\n",
    "    return (inp, parsed['y'])\n",
    "\n",
    "def input_parser_test(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "                           'input_ids': bert_feature,\n",
    "                           'token_type_ids': bert_feature,\n",
    "                           'attention_mask': bert_feature,\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids': parsed['input_ids'],\n",
    "           'token_type_ids': parsed['token_type_ids'],\n",
    "           'attention_mask': parsed['attention_mask']}\n",
    "    return (parsed['article_id'][0], inp, parsed['y'][0])\n",
    "\n",
    "def input_parser_cs(example):\n",
    "    feature_description = {'input_ids1': bert_feature,\n",
    "                           'token_type_ids1': bert_feature,\n",
    "                           'attention_mask1': bert_feature,\n",
    "                           'input_ids2': bert_feature,\n",
    "                           'token_type_ids2': bert_feature,\n",
    "                           'attention_mask2': bert_feature,\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids1': parsed['input_ids1'],\n",
    "           'token_type_ids1': parsed['token_type_ids1'],\n",
    "           'attention_mask1': parsed['attention_mask1'],\n",
    "           'input_ids2': parsed['input_ids2'],\n",
    "           'token_type_ids2': parsed['token_type_ids2'],\n",
    "           'attention_mask2': parsed['attention_mask2']}\n",
    "    return (inp, parsed['y'])\n",
    "\n",
    "train_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TRAIN_BERT_BASE', input_parser_train)\n",
    "train_sampling_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TRAIN_BERT_SAMPLING', input_parser_cs)\n",
    "test_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TEST_BERT_BASE', input_parser_test)\n",
    "\n",
    "num_train_examples = int(DATASET_SIZE * DATASET_DEV_SPLIT)\n",
    "train_ds_split = train_dataset.take(num_train_examples)\n",
    "train_ds_split = train_ds_split.shuffle(100, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "dev_ds_split = train_dataset.skip(num_train_examples).batch(BATCH_SIZE)\n",
    "\n",
    "train_sampling_dataset_size = int(DATASET_SIZE* NUM_SAMPLING_CANDIDATES * DATASET_DEV_SPLIT)\n",
    "train_sampling_dataset = train_sampling_dataset.map(lambda inp, y: (inp, {'out_s1': y,'out_diff': y}))\n",
    "# use half the batch size because of memory concerns\n",
    "train_sampling_dataset_split = train_sampling_dataset.take(train_sampling_dataset_size).batch(int(BATCH_SIZE / 2)).prefetch(1)\n",
    "dev_sampling_dataset = train_sampling_dataset.skip(train_sampling_dataset_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-component",
   "metadata": {},
   "source": [
    "### Load ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sixth-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../models/bert-base-german-cased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f51703d7580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f51703d7580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "\n",
    "def cbert_model_forward(inp):\n",
    "    outputs = cbert_model(inp)\n",
    "    return outputs.logits\n",
    "\n",
    "shape=(MAX_LEN,)\n",
    "input_type=tf.int32\n",
    "\n",
    "input_ids1 = Input(shape=shape, name='input_ids1', dtype=input_type)\n",
    "attention_mask1 = Input(shape=shape, name='attention_mask1', dtype=input_type)\n",
    "token_type_ids1 = Input(shape=shape, name='token_type_ids1', dtype=input_type)\n",
    "\n",
    "input_ids2 = Input(shape=shape, name='input_ids2', dtype=input_type)\n",
    "attention_mask2 = Input(shape=shape, name='attention_mask2', dtype=input_type)\n",
    "token_type_ids2 = Input(shape=shape, name='token_type_ids2', dtype=input_type) \n",
    "\n",
    "cbert_model_ranking = build_ranking_model(cbert_model_forward,\n",
    "                                          [input_ids1, attention_mask1, token_type_ids1],\n",
    "                                          [input_ids2, attention_mask2, token_type_ids2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-milton",
   "metadata": {},
   "source": [
    "### Training without ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "precision = tf.keras.metrics.Precision(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "recall = tf.keras.metrics.Recall(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "binacc = tf.keras.metrics.BinaryAccuracy(threshold=BINACC_THRESHOLD)\n",
    "metrics = [precision, recall, binacc]\n",
    "cbert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model.fit(train_ds_split,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=dev_ds_split,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-placement",
   "metadata": {},
   "source": [
    "### Training with ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "precision = tf.keras.metrics.Precision(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "recall = tf.keras.metrics.Recall(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "binacc = tf.keras.metrics.BinaryAccuracy(threshold=BINACC_THRESHOLD)\n",
    "metrics = {'out_s1': [precision, recall, binacc]}\n",
    "cbert_model_ranking.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED_RANKING, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model_ranking.fit(train_sampling_dataset_split,\n",
    "                epochs=1,\n",
    "                validation_data=dev_sampling_dataset,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "behind-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = cbert_model_ranking.get_layer(name='tf_bert_for_sequence_classification')\n",
    "cbert_model.save_weights(MODEL_PATH_BERT_TUNED_RANKING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-september",
   "metadata": {},
   "source": [
    "### Load fine-tuned BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "descending-sister",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1cc83c32e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "cbert_model.load_weights(MODEL_PATH_BERT_TUNED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-assumption",
   "metadata": {},
   "source": [
    "### Load fine-tuned BERT model + ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "democratic-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../models/bert-base-german-cased/ were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../models/bert-base-german-cased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f98756c5e20>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "cbert_model.load_weights(MODEL_PATH_BERT_TUNED_RANKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "wanted-raise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f98f5423580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f98f5423580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "BERT/MAP: 0.46114418386081657\n",
      "BERT/P@1: 0.4020618556701031\n",
      "BERT/P@5: 0.21649484536082456\n",
      "BERT/P@10: 0.15474963181148732\n"
     ]
    }
   ],
   "source": [
    "def prediction_func(inps):\n",
    "    outputs = cbert_model.predict(inps)\n",
    "    return [l[0] for l in outputs.logits]\n",
    "\n",
    "eval_data_bert = batch_predict(test_dataset, 100, prediction_func)\n",
    "print('BERT/MAP: {}'.format(mean_average_precision(eval_data_bert)))\n",
    "for k in [1, 5, 10]:\n",
    "\n",
    "    print('BERT/P@{}: {}'.format(k, precision_at_k(eval_data_bert, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-breeding",
   "metadata": {},
   "source": [
    "### Hyperparamerters\n",
    "\n",
    "|     | BERT BASE | BERT SAMPLING\n",
    "|-----|---------|---|\n",
    "| BATCH_SIZE |  32  | 16\n",
    "| EPOCHS |     5    | 1\n",
    "\n",
    "### Results\n",
    "|     | BERT BASE | BERT SAMPLING\n",
    "|-----|---------|---|\n",
    "| MAP |  0.47965098263440786    | 0.46114418386081657 |\n",
    "| P@1 |     0.42857142857142855    | 0.4020618556701031 |\n",
    "| P@5 |     0.18571428571428555    | 0.21649484536082456|\n",
    "| P@10 |     0.14285714285714268    | 0.15474963181148732 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
