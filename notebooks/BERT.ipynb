{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from evaluation import mean_average_precision, precision_at_k\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "    \n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "DATA_PATH_PROCESSED = '../data/GermanFakeNC_PROCESSED'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "MODEL_PATH_BERT_TUNED = '../models/bert-base-german-cased-tuned/checkpoint.ckpt'\n",
    "MODEL_PATH_BERT_TUNED_RANKING = '../models/bert-base-german-cased-tuned-ranking/checkpoint.ckpt'\n",
    "MODEL_PATH_BERT_TUNED_TRUENEWS = '../models/bert-base-german-cased-tuned-truenews/checkpoint.ckpt'\n",
    "DATASET_SIZE = 14765\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "NUM_SAMPLING_CANDIDATES = 5\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 134\n",
    "LEARNING_RATE = 5e-5\n",
    "BINACC_THRESHOLD = 0.1\n",
    "PRECISION_RECALL_THRESHOLDS = [0.05, 0.1, 0.2, 0.5]\n",
    "EPOCHS = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_feature = tf.io.FixedLenFeature([MAX_LEN], dtype=tf.int64)\n",
    "\n",
    "def input_parser_train(example):\n",
    "    feature_description = {'input_ids': bert_feature,\n",
    "                           'token_type_ids': bert_feature,\n",
    "                           'attention_mask': bert_feature,\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids': parsed['input_ids'],\n",
    "           'token_type_ids': parsed['token_type_ids'],\n",
    "           'attention_mask': parsed['attention_mask'],}\n",
    "    return (inp, parsed['y'])\n",
    "\n",
    "def input_parser_test(example):\n",
    "    feature_description = {'article_id': tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "                           'input_ids': bert_feature,\n",
    "                           'token_type_ids': bert_feature,\n",
    "                           'attention_mask': bert_feature,\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids': parsed['input_ids'],\n",
    "           'token_type_ids': parsed['token_type_ids'],\n",
    "           'attention_mask': parsed['attention_mask']}\n",
    "    return (parsed['article_id'][0], inp, parsed['y'][0])\n",
    "\n",
    "def input_parser_cs(example):\n",
    "    feature_description = {'input_ids1': bert_feature,\n",
    "                           'token_type_ids1': bert_feature,\n",
    "                           'attention_mask1': bert_feature,\n",
    "                           'input_ids2': bert_feature,\n",
    "                           'token_type_ids2': bert_feature,\n",
    "                           'attention_mask2': bert_feature,\n",
    "                           'y': tf.io.FixedLenFeature([1], dtype=tf.float32)}\n",
    "\n",
    "    parsed = tf.io.parse_single_example(example, feature_description)\n",
    "    inp = {'input_ids1': parsed['input_ids1'],\n",
    "           'token_type_ids1': parsed['token_type_ids1'],\n",
    "           'attention_mask1': parsed['attention_mask1'],\n",
    "           'input_ids2': parsed['input_ids2'],\n",
    "           'token_type_ids2': parsed['token_type_ids2'],\n",
    "           'attention_mask2': parsed['attention_mask2']}\n",
    "    return (inp, parsed['y'])\n",
    "\n",
    "def format_ranking_dataset(dataset):    \n",
    "    train_dataset_size = int(DATASET_SIZE * NUM_SAMPLING_CANDIDATES * DATASET_DEV_SPLIT)\n",
    "    train_dataset = train_sampling_dataset.map(lambda inp, y: (inp, {'out_s1': y,'out_diff': y}))\n",
    "    # use half the batch size because of memory concerns\n",
    "    train_dataset_split = train_sampling_dataset.take(train_dataset_size).batch(int(BATCH_SIZE / 2)).prefetch(1)\n",
    "    dev_dataset = train_sampling_dataset.skip(train_dataset_size).batch(BATCH_SIZE)    \n",
    "    return train_dataset_split, dev_dataset\n",
    "\n",
    "train_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TRAIN_BERT_BASE', input_parser_train)\n",
    "train_sampling_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TRAIN_BERT_SAMPLING', input_parser_cs)\n",
    "train_truenews_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TRAIN_BERT_TRUENEWS', input_parser_cs)\n",
    "test_dataset = read_tfrecords(DATA_PATH_PROCESSED, 'TEST_BERT_BASE', input_parser_test)\n",
    "\n",
    "num_train_examples = int(DATASET_SIZE * DATASET_DEV_SPLIT)\n",
    "train_ds_split = train_dataset.take(num_train_examples)\n",
    "train_ds_split = train_ds_split.shuffle(100, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "dev_ds_split = train_dataset.skip(num_train_examples).batch(BATCH_SIZE)\n",
    "\n",
    "train_sampling_dataset_split, dev_sampling_dataset = format_ranking_dataset(train_sampling_dataset)\n",
    "train_truenews_dataset_split, dev_truenews_dataset = format_ranking_dataset(train_sampling_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-benjamin",
   "metadata": {},
   "source": [
    "### Load base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-jordan",
   "metadata": {},
   "source": [
    "### Load ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "\n",
    "def cbert_model_forward(inp):\n",
    "    outputs = cbert_model(inp)\n",
    "    return outputs.logits\n",
    "\n",
    "shape=(MAX_LEN,)\n",
    "input_type=tf.int32\n",
    "\n",
    "input_ids1 = Input(shape=shape, name='input_ids1', dtype=input_type)\n",
    "attention_mask1 = Input(shape=shape, name='attention_mask1', dtype=input_type)\n",
    "token_type_ids1 = Input(shape=shape, name='token_type_ids1', dtype=input_type)\n",
    "\n",
    "input_ids2 = Input(shape=shape, name='input_ids2', dtype=input_type)\n",
    "attention_mask2 = Input(shape=shape, name='attention_mask2', dtype=input_type)\n",
    "token_type_ids2 = Input(shape=shape, name='token_type_ids2', dtype=input_type) \n",
    "\n",
    "cbert_model_ranking = build_ranking_model(cbert_model_forward,\n",
    "                                          [input_ids1, attention_mask1, token_type_ids1],\n",
    "                                          [input_ids2, attention_mask2, token_type_ids2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-fields",
   "metadata": {},
   "source": [
    "### Training without ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "precision = tf.keras.metrics.Precision(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "recall = tf.keras.metrics.Recall(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "binacc = tf.keras.metrics.BinaryAccuracy(threshold=BINACC_THRESHOLD)\n",
    "metrics = [precision, recall, binacc]\n",
    "cbert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model.fit(train_ds_split,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=dev_ds_split,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-douglas",
   "metadata": {},
   "source": [
    "### Training with ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "precision = tf.keras.metrics.Precision(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "recall = tf.keras.metrics.Recall(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "binacc = tf.keras.metrics.BinaryAccuracy(threshold=BINACC_THRESHOLD)\n",
    "metrics = {'out_s1': [precision, recall, binacc]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model_ranking.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED_RANKING, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model_ranking.fit(train_sampling_dataset_split,\n",
    "                epochs=1,\n",
    "                validation_data=dev_sampling_dataset,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = cbert_model_ranking.get_layer(name='tf_bert_for_sequence_classification')\n",
    "cbert_model.save_weights(MODEL_PATH_BERT_TUNED_RANKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model_truenews = cbert_model_ranking\n",
    "cbert_model_truenews.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED_TRUENEWS, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model_truenews.fit(train_truenews_dataset_split,\n",
    "                epochs=1,\n",
    "                validation_data=dev_truenews_dataset,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = cbert_model_truenews.get_layer(name='tf_bert_for_sequence_classification')\n",
    "cbert_model.save_weights(MODEL_PATH_BERT_TUNED_TRUENEWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-headquarters",
   "metadata": {},
   "source": [
    "### Load fine-tuned BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "cbert_model.load_weights(MODEL_PATH_BERT_TUNED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-burton",
   "metadata": {},
   "source": [
    "### Load fine-tuned BERT model + ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "cbert_model.load_weights(MODEL_PATH_BERT_TUNED_RANKING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-mistress",
   "metadata": {},
   "source": [
    "### Load fine-tuned BERT model + truenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "cbert_model.load_weights(MODEL_PATH_BERT_TUNED_TRUENEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_func(inps):\n",
    "    outputs = cbert_model.predict(inps)\n",
    "    return [l[0] for l in outputs.logits]\n",
    "\n",
    "eval_data_bert = batch_predict(test_dataset, 100, prediction_func)\n",
    "print('BERT/MAP: {}'.format(mean_average_precision(eval_data_bert)))\n",
    "for k in [1, 5, 10]:\n",
    "\n",
    "    print('BERT/P@{}: {}'.format(k, precision_at_k(eval_data_bert, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-southwest",
   "metadata": {},
   "source": [
    "### Hyperparamerters\n",
    "\n",
    "|     | BERT BASE | BERT SAMPLING | BERT TRUENEWS |\n",
    "|-----|---------|---|---|\n",
    "| BATCH_SIZE |  32  | 16 | 16 |\n",
    "| EPOCHS |     5    | 1 | 1 |\n",
    "\n",
    "### Results\n",
    "|     | BERT BASE | BERT SAMPLING | BERT TRUENEWS |\n",
    "|-----|---------|---|---|\n",
    "| MAP |  0.47965098263440786    | 0.46114418386081657 | 0.3324119192396223 |\n",
    "| P@1 |     0.42857142857142855    | 0.4020618556701031 | 0.21649484536082475 |\n",
    "| P@5 |     0.18571428571428555    | 0.21649484536082456| 0.13608247422680408 |\n",
    "| P@10 |     0.14285714285714268    | 0.15474963181148732 | 0.12072901325478631 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
