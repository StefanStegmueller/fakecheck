{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "quick-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.4.1\n",
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# workaround to import local modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "from evaluation import mean_average_precision, precision_at_k\n",
    "from utils import *\n",
    "from model import *\n",
    "\n",
    "print('Tensorflow Version: {}'.format(tf.__version__))\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "    \n",
    "DATA_PATH_FORMATED_TRAIN = '../data/GermanFakeNC_FORMATED_TRAIN.json'\n",
    "DATA_PATH_FORMATED_TEST = '../data/GermanFakeNC_FORMATED_TEST.json'\n",
    "MODEL_PATH_BERT = '../models/bert-base-german-cased/'\n",
    "MODEL_PATH_BERT_TUNED = '../models/bert-base-german-cased-tuned/checkpoint.ckpt'\n",
    "DATASET_DEV_SPLIT = 0.8\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 128\n",
    "LEARNING_RATE = 5e-5\n",
    "BINACC_THRESHOLD = 0.1\n",
    "PRECISION_RECALL_THRESHOLDS = [0.05, 0.1, 0.2, 0.5]\n",
    "EPOCHS = 5\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alleged-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path) as json_file:\n",
    "            return json.load(json_file)\n",
    "        \n",
    "def encode(sentences):\n",
    "    return tokenizer(sentences, max_length=MAX_LEN, truncation=True, padding=True, return_tensors='tf')\n",
    "        \n",
    "def to_dataset(data):\n",
    "    sentences = [d['org'] for d in data]\n",
    "    encodings_ds = tf.data.Dataset.from_tensor_slices(encode(sentences))    \n",
    "    encodings_ds = encodings_ds.map(lambda ex: {i:ex[i] for i in ex}) # Batch encoding to dictionary\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices([d['lbl'] for d in data]).map(lambda lbl: tf.reshape(lbl, [1]))\n",
    "    ids_ds = tf.data.Dataset.from_tensor_slices([d['article_id'] for d in data])\n",
    "    return tf.data.Dataset.zip((ids_ds, encodings_ds, labels_ds))\n",
    "        \n",
    "train_data = read_data(DATA_PATH_FORMATED_TRAIN)\n",
    "test_data = read_data(DATA_PATH_FORMATED_TEST)\n",
    "    \n",
    "train_ds = to_dataset(train_data).map(lambda ida, inp, lbl: (inp, lbl))\n",
    "test_ds = to_dataset(test_data).map(lambda ida, inp, lbl: (ida, inp, lbl[0]))\n",
    "\n",
    "num_train_examples = int(len(train_data) * DATASET_DEV_SPLIT)\n",
    "train_ds_split = train_ds.take(num_train_examples)\n",
    "train_ds_split = train_ds_split.shuffle(100, reshuffle_each_iteration=True).batch(BATCH_SIZE)\n",
    "dev_ds_split = train_ds.skip(num_train_examples).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-alexander",
   "metadata": {},
   "source": [
    "### Load initial pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../models/bert-base-german-cased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "trained-joint",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../models/bert-base-german-cased/ were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../models/bert-base-german-cased/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "cbert_model = load_bert_model(MODEL_PATH_BERT)\n",
    "\n",
    "def cbert_model_forward(inp):\n",
    "    outputs = cbert_model(inp)\n",
    "    return outputs.logits\n",
    "\n",
    "shape=(MAX_LEN,)\n",
    "input_type=tf.int32\n",
    "\n",
    "input_ids1 = Input(shape=shape, name='input_ids1', dtype=input_type)\n",
    "attention_mask1 = Input(shape=shape, name='attention_mask1', dtype=input_type)\n",
    "token_type_ids1 = Input(shape=shape, name='token_type_ids1', dtype=input_type)\n",
    "\n",
    "input_ids2 = Input(shape=shape, name='input_ids2', dtype=input_type)\n",
    "attention_mask2 = Input(shape=shape, name='attention_mask2', dtype=input_type)\n",
    "token_type_ids2 = Input(shape=shape, name='token_type_ids2', dtype=input_type) \n",
    "\n",
    "cbert_ranking_model = build_ranking_model(cbert_model_forward,\n",
    "                                          [input_ids1, attention_mask1, token_type_ids1],\n",
    "                                          [input_ids2, attention_mask2, token_type_ids2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-involvement",
   "metadata": {},
   "source": [
    "### Load fine-tuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minute-remainder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1cc83c32e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbert_model.load_weights(MODEL_PATH_BERT_TUNED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-palace",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fossil-swaziland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 195), started 0:47:09 ago. (Use '!kill 195' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1026c58c3a3c4da1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1026c58c3a3c4da1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f01d8234580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f01d8234580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2793 - precision: 0.0945 - recall: 0.3186 - binary_accuracy: 0.6998 WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "276/276 [==============================] - 7525s 27s/step - loss: 0.2792 - precision: 0.0947 - recall: 0.3187 - binary_accuracy: 0.6999 - val_loss: 0.2072 - val_precision: 0.1832 - val_recall: 0.3938 - val_binary_accuracy: 0.8354\n",
      "\n",
      "Epoch 00001: val_binary_accuracy improved from -inf to 0.83545, saving model to ../models/bert-base-german-cased-tuned/checkpoint.ckpt\n",
      "Epoch 2/5\n",
      "276/276 [==============================] - 7074s 26s/step - loss: 0.2205 - precision: 0.2818 - recall: 0.4326 - binary_accuracy: 0.8038 - val_loss: 0.2157 - val_precision: 0.2448 - val_recall: 0.5514 - val_binary_accuracy: 0.7629\n",
      "\n",
      "Epoch 00002: val_binary_accuracy did not improve from 0.83545\n",
      "Epoch 3/5\n",
      "276/276 [==============================] - 7081s 26s/step - loss: 0.1655 - precision: 0.4503 - recall: 0.6491 - binary_accuracy: 0.8571 - val_loss: 0.2615 - val_precision: 0.2417 - val_recall: 0.4401 - val_binary_accuracy: 0.8513\n",
      "\n",
      "Epoch 00003: val_binary_accuracy improved from 0.83545 to 0.85131, saving model to ../models/bert-base-german-cased-tuned/checkpoint.ckpt\n",
      "Epoch 4/5\n",
      "276/276 [==============================] - 7079s 26s/step - loss: 0.1433 - precision: 0.5366 - recall: 0.7188 - binary_accuracy: 0.9185 - val_loss: 0.2678 - val_precision: 0.3305 - val_recall: 0.3151 - val_binary_accuracy: 0.9075\n",
      "\n",
      "Epoch 00004: val_binary_accuracy improved from 0.85131 to 0.90752, saving model to ../models/bert-base-german-cased-tuned/checkpoint.ckpt\n",
      "Epoch 5/5\n",
      "276/276 [==============================] - 7077s 26s/step - loss: 0.0769 - precision: 0.7320 - recall: 0.8736 - binary_accuracy: 0.9633 - val_loss: 0.2822 - val_precision: 0.2595 - val_recall: 0.2825 - val_binary_accuracy: 0.8835\n",
      "\n",
      "Epoch 00005: val_binary_accuracy did not improve from 0.90752\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "precision = tf.keras.metrics.Precision(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "recall = tf.keras.metrics.Recall(thresholds=PRECISION_RECALL_THRESHOLDS)\n",
    "binacc = tf.keras.metrics.BinaryAccuracy(threshold=BINACC_THRESHOLD)\n",
    "metrics = [precision, recall, binacc]\n",
    "cbert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "checkpoint_callback = get_checkpoint_callback(MODEL_PATH_BERT_TUNED, 'val_binary_accuracy', weights_only=True)\n",
    "tensorboard_callback = get_tensorboard_callback('logs')\n",
    "\n",
    "%tensorboard --logdir logs --bind_all\n",
    "history = cbert_model.fit(train_ds_split,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=dev_ds_split,\n",
    "                callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chicken-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1cc83d3580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f1cc83d3580>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "BERT/MAP: 0.47965098263440786\n",
      "Ranking/P@1: 0.42857142857142855\n",
      "Ranking/P@5: 0.18571428571428555\n",
      "Ranking/P@10: 0.14285714285714268\n"
     ]
    }
   ],
   "source": [
    "def prediction_func(inps):\n",
    "    outputs = cbert_model.predict(inps)\n",
    "    return [l[0] for l in outputs.logits]\n",
    "\n",
    "eval_data_bert = batch_predict(test_ds, 100, prediction_func)\n",
    "print('BERT/MAP: {}'.format(mean_average_precision(eval_data_bert)))\n",
    "for k in [1, 5, 10]:\n",
    "    print('BERT/P@{}: {}'.format(k, precision_at_k(eval_data_bert, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-category",
   "metadata": {},
   "source": [
    "### Results\n",
    "|     | BERT 2 Epochs| BERT 5 Epochs |\n",
    "|-----|------|---------|\n",
    "| MAP |   0.45336887554833294   |    0.47965098263440786    |\n",
    "| P@1 |      |     0.42857142857142855    |\n",
    "| P@5 |      |     0.18571428571428555    |\n",
    "| P@10 |      |     0.14285714285714268    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-interval",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
